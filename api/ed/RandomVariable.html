<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward – ed.RandomVariable</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  /* Downgrade API's header styles without explicitly modifying their type. */
  div.nine.columns h1 { text-align: left; }
  div.nine.columns h1 { font-size: 3.0rem; line-height: 1.25; letter-spacing: -.1rem; }
  div.nine.columns h2 { font-size: 2.4rem; line-height: 1.3;  letter-spacing: -.1rem; }
  div.nine.columns h3 { font-size: 2.4rem; line-height: 1.35; letter-spacing: -.08rem; }
  div.nine.columns h4 { font-size: 1.8rem; line-height: 1.5;  letter-spacing: -.05rem; }
  div.nine.columns h5 { font-size: 1.5rem; line-height: 1.6;  letter-spacing: 0; }
  @media (min-width: 550px) {
    div.nine.columns h1 { font-size: 3.6rem; }
    div.nine.columns h2 { font-size: 3.0rem; }
    div.nine.columns h3 { font-size: 3.0rem; }
    div.nine.columns h4 { font-size: 2.5rem; }
    div.nine.columns h5 { font-size: 1.5rem; }
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/api/">API</a>
    <hr style="margin-top: 1rem; margin-bottom: 1.5rem;"/>
    <a class="button u-full-width" href="/api/reference">Reference</a>
    <a class="button u-full-width" href="/api/ed/criticisms">ed.criticisms</a>
<a class="button u-full-width" href="/api/ed/inferences">ed.inferences</a>
<a class="button u-full-width" href="/api/ed/models">ed.models</a>
<a class="button u-full-width" href="/api/ed/util">ed.util</a>
<a class="button u-full-width" href="/api/observations">observations</a>

    <div class="row" style="padding-bottom: 5%"> </div>
    <a class="button2 u-pull-right" style="padding-right:10%"
      href="https://github.com/blei-lab/edward">
      <span style="vertical-align:middle;">Github</span>&nbsp;
      <img src="/images/github-mark.svg" style="vertical-align:middle;"
      alt="Edward on Github" />
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">

<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="ed.RandomVariable" />
<meta itemprop="property" content="sample_shape"/>
<meta itemprop="property" content="shape"/>
<meta itemprop="property" content="__abs__"/>
<meta itemprop="property" content="__add__"/>
<meta itemprop="property" content="__and__"/>
<meta itemprop="property" content="__bool__"/>
<meta itemprop="property" content="__div__"/>
<meta itemprop="property" content="__eq__"/>
<meta itemprop="property" content="__floordiv__"/>
<meta itemprop="property" content="__ge__"/>
<meta itemprop="property" content="__getitem__"/>
<meta itemprop="property" content="__gt__"/>
<meta itemprop="property" content="__init__"/>
<meta itemprop="property" content="__invert__"/>
<meta itemprop="property" content="__iter__"/>
<meta itemprop="property" content="__le__"/>
<meta itemprop="property" content="__lt__"/>
<meta itemprop="property" content="__matmul__"/>
<meta itemprop="property" content="__mod__"/>
<meta itemprop="property" content="__mul__"/>
<meta itemprop="property" content="__neg__"/>
<meta itemprop="property" content="__nonzero__"/>
<meta itemprop="property" content="__or__"/>
<meta itemprop="property" content="__pow__"/>
<meta itemprop="property" content="__radd__"/>
<meta itemprop="property" content="__rand__"/>
<meta itemprop="property" content="__rdiv__"/>
<meta itemprop="property" content="__rfloordiv__"/>
<meta itemprop="property" content="__rmatmul__"/>
<meta itemprop="property" content="__rmod__"/>
<meta itemprop="property" content="__rmul__"/>
<meta itemprop="property" content="__ror__"/>
<meta itemprop="property" content="__rpow__"/>
<meta itemprop="property" content="__rsub__"/>
<meta itemprop="property" content="__rtruediv__"/>
<meta itemprop="property" content="__rxor__"/>
<meta itemprop="property" content="__sub__"/>
<meta itemprop="property" content="__truediv__"/>
<meta itemprop="property" content="__xor__"/>
<meta itemprop="property" content="eval"/>
<meta itemprop="property" content="get_ancestors"/>
<meta itemprop="property" content="get_blanket"/>
<meta itemprop="property" content="get_children"/>
<meta itemprop="property" content="get_descendants"/>
<meta itemprop="property" content="get_parents"/>
<meta itemprop="property" content="get_shape"/>
<meta itemprop="property" content="get_siblings"/>
<meta itemprop="property" content="get_variables"/>
<meta itemprop="property" content="value"/>
<meta itemprop="property" content="__array_priority__"/>
</div>
<h1 id="ed.randomvariable">ed.RandomVariable</h1>
<h2 id="class-randomvariable">Class <code>RandomVariable</code></h2>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>ed.RandomVariable</code></li>
<li>Class <code>ed.models.RandomVariable</code></li>
</ul>
<p>Defined in <a href="https://github.com/blei-lab/edward/tree/master/edward/models/random_variable.py"><code>edward/models/random_variable.py</code></a>.</p>
<p>Base class for random variables.</p>
<p>A random variable is an object parameterized by tensors. It is equipped with methods such as the log-density, mean, and sample.</p>
<p>It also wraps a tensor, where the tensor corresponds to a sample from the random variable. This enables operations on the TensorFlow graph, allowing random variables to be used in conjunction with other TensorFlow ops.</p>
<p>The random variable’s shape is given by</p>
<p><code>sample_shape + batch_shape + event_shape</code>,</p>
<p>where <code>sample_shape</code> is an optional argument representing the dimensions of samples drawn from the distribution (default is a scalar); <code>batch_shape</code> is the number of independent random variables (determined by the shape of its parameters); and <code>event_shape</code> is the shape of one draw from the distribution (e.g., <code>Normal</code> has a scalar <code>event_shape</code>; <code>Dirichlet</code> has a vector <code>event_shape</code>).</p>
<h4 id="notes">Notes</h4>
<p><code>RandomVariable</code> assumes use in a multiple inheritance setting. The child class must first inherit <code>RandomVariable</code>, then second inherit a class in <code>tf.contrib.distributions</code>. With Python’s method resolution order, this implies the following during initialization (using <code>distributions.Bernoulli</code> as an example):</p>
<ol type="1">
<li>Start the <code>__init__()</code> of the child class, which passes all <code>*args, **kwargs</code> to <code>RandomVariable</code>.</li>
<li>This in turn passes all <code>*args, **kwargs</code> to <code>distributions.Bernoulli</code>, completing the <code>__init__()</code> of <code>distributions.Bernoulli</code>.</li>
<li>Complete the <code>__init__()</code> of <code>RandomVariable</code>, which calls <code>self.sample()</code>, relying on the method from <code>distributions.Bernoulli</code>.</li>
<li>Complete the <code>__init__()</code> of the child class.</li>
</ol>
<p>Methods from both <code>RandomVariable</code> and <code>distributions.Bernoulli</code> populate the namespace of the child class. Methods from <code>RandomVariable</code> will take higher priority if there are conflicts.</p>
<h4 id="examples">Examples</h4>
<pre class="python"><code>p = tf.constant(0.5)
x = Bernoulli(p)

z1 = tf.constant([[1.0, -0.8], [0.3, -1.0]])
z2 = tf.constant([[0.9, 0.2], [2.0, -0.1]])
x = Bernoulli(logits=tf.matmul(z1, z2))

mu = Normal(tf.constant(0.0), tf.constant(1.0))
x = Normal(mu, tf.constant(1.0))</code></pre>
<h2 id="properties">Properties</h2>
<h3 id="sample_shape">
<code>sample_shape</code>
</h3>
<p>Sample shape of random variable.</p>
<h3 id="shape">
<code>shape</code>
</h3>
<p>Shape of random variable.</p>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<pre class="python"><code>__init__(
    *args,
    **kwargs
)</code></pre>
<p>Create a new random variable.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>sample_shape</code></b>: tf.TensorShape. Shape of samples to draw from the random variable.</li>
<li><b><code>value</code></b>: tf.Tensor. Fixed tensor to associate with random variable. Must have shape <code>sample_shape + batch_shape + event_shape</code>.</li>
<li><b><code>collections</code></b>: list. Optional list of graph collections (lists). The random variable is added to these collections. Defaults to <code>[ed.random_variables()]</code>.</li>
</ul>
<h3 id="__abs__">
<code><strong>abs</strong></code>
</h3>
<pre class="python"><code>__abs__(
    a,
    *args
)</code></pre>
<p>Computes the absolute value of a tensor.</p>
<p>Given a tensor <code>x</code> of complex numbers, this operation returns a tensor of type <code>float32</code> or <code>float64</code> that is the absolute value of each element in <code>x</code>. All elements in <code>x</code> must be complex numbers of the form \(a + bj\). The absolute value is computed as \( \). For example:</p>
<pre class="python"><code>x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])
tf.abs(x)  # [5.25594902, 6.60492229]</code></pre>
<h4 id="args-1">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> or <code>SparseTensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code> or <code>complex128</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A <code>Tensor</code> or <code>SparseTensor</code> the same size and type as <code>x</code> with absolute values. Note, for <code>complex64</code> or <code>complex128</code> input, the returned <code>Tensor</code> will be of type <code>float32</code> or <code>float64</code>, respectively.</p>
<h3 id="__add__">
<code><strong>add</strong></code>
</h3>
<pre class="python"><code>__add__(
    a,
    *args
)</code></pre>
<p>Returns x + y element-wise.</p>
<p><em>NOTE</em>: <code>Add</code> supports broadcasting. <code>AddN</code> does not. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-2">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>bfloat16</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>, <code>string</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-1">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__and__">
<code><strong>and</strong></code>
</h3>
<pre class="python"><code>__and__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of x AND y element-wise.</p>
<p><em>NOTE</em>: <code>LogicalAnd</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-3">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-2">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__bool__">
<code><strong>bool</strong></code>
</h3>
<pre class="python"><code>__bool__()</code></pre>
<h3 id="__div__">
<code><strong>div</strong></code>
</h3>
<pre class="python"><code>__div__(
    a,
    *args
)</code></pre>
<p>Divide two values using Python 2 semantics. Used for Tensor.__div__.</p>
<h4 id="args-4">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li>
<li><b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-3">Returns:</h4>
<p><code>x / y</code> returns the quotient of x and y.</p>
<h3 id="__eq__">
<code><strong>eq</strong></code>
</h3>
<pre class="python"><code>__eq__(other)</code></pre>
<h3 id="__floordiv__">
<code><strong>floordiv</strong></code>
</h3>
<pre class="python"><code>__floordiv__(
    a,
    *args
)</code></pre>
<p>Divides <code>x / y</code> elementwise, rounding toward the most negative integer.</p>
<p>The same as <code>tf.div(x,y)</code> for integers, but uses <code>tf.floor(tf.div(x,y))</code> for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by <code>x // y</code> floor division in Python 3 and in Python 2.7 with <code>from __future__ import division</code>.</p>
<p>Note that for efficiency, <code>floordiv</code> uses C semantics for negative numbers (unlike Python and Numpy).</p>
<p><code>x</code> and <code>y</code> must have the same type, and the result will have the same type as well.</p>
<h4 id="args-5">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li>
<li><b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-4">Returns:</h4>
<p><code>x / y</code> rounded down (except possibly towards zero for negative integers).</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If the inputs are complex.</li>
</ul>
<h3 id="__ge__">
<code><strong>ge</strong></code>
</h3>
<pre class="python"><code>__ge__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of (x &gt;= y) element-wise.</p>
<p><em>NOTE</em>: <code>GreaterEqual</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-6">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>int64</code>, <code>bfloat16</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-5">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__getitem__">
<code><strong>getitem</strong></code>
</h3>
<pre class="python"><code>__getitem__(
    a,
    *args
)</code></pre>
<p>Overload for Tensor.__getitem__.</p>
<p>This operation extracts the specified region from the tensor. The notation is similar to NumPy with the restriction that currently only support basic indexing. That means that using a non-scalar tensor as input is not currently allowed.</p>
<p>Some useful examples:</p>
<pre class="python"><code># strip leading and trailing 2 elements
foo = tf.constant([1,2,3,4,5,6])
print(foo[2:-2].eval())  # =&gt; [3,4]

# skip every row and reverse every column
foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])
print(foo[::2,::-1].eval())  # =&gt; [[3,2,1], [9,8,7]]

# Use scalar tensors as indices on both dimensions
print(foo[tf.constant(0), tf.constant(2)].eval())  # =&gt; 3

# Insert another dimension
foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])
print(foo[tf.newaxis, :, :].eval()) # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]
print(foo[:, tf.newaxis, :].eval()) # =&gt; [[[1,2,3]], [[4,5,6]], [[7,8,9]]]
print(foo[:, :, tf.newaxis].eval()) # =&gt; [[[1],[2],[3]], [[4],[5],[6]],
[[7],[8],[9]]]

# Ellipses (3 equivalent operations)
foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])
print(foo[tf.newaxis, :, :].eval())  # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]
print(foo[tf.newaxis, ...].eval())  # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]
print(foo[tf.newaxis].eval())  # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]</code></pre>
<p>Notes: - <code>tf.newaxis</code> is <code>None</code> as in NumPy. - An implicit ellipsis is placed at the end of the <code>slice_spec</code> - NumPy advanced indexing is currently not supported.</p>
<h4 id="args-7">Args:</h4>
<ul>
<li><b><code>tensor</code></b>: An ops.Tensor object.</li>
<li><b><code>slice_spec</code></b>: The arguments to Tensor.__getitem__.</li>
<li><b><code>var</code></b>: In the case of variable slice assignment, the Variable object to slice (i.e. tensor is the read-only view of this variable).</li>
</ul>
<h4 id="returns-6">Returns:</h4>
<p>The appropriate slice of “tensor”, based on “slice_spec”.</p>
<h4 id="raises-1">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If a slice range is negative size.</li>
<li><b><code>TypeError</code></b>: If the slice indices aren’t int, slice, or Ellipsis.</li>
</ul>
<h3 id="__gt__">
<code><strong>gt</strong></code>
</h3>
<pre class="python"><code>__gt__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of (x &gt; y) element-wise.</p>
<p><em>NOTE</em>: <code>Greater</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-8">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>int64</code>, <code>bfloat16</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-7">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__invert__">
<code><strong>invert</strong></code>
</h3>
<pre class="python"><code>__invert__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of NOT x element-wise.</p>
<h4 id="args-9">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-8">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__iter__">
<code><strong>iter</strong></code>
</h3>
<pre class="python"><code>__iter__()</code></pre>
<h3 id="__le__">
<code><strong>le</strong></code>
</h3>
<pre class="python"><code>__le__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of (x &lt;= y) element-wise.</p>
<p><em>NOTE</em>: <code>LessEqual</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-10">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>int64</code>, <code>bfloat16</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-9">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__lt__">
<code><strong>lt</strong></code>
</h3>
<pre class="python"><code>__lt__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of (x &lt; y) element-wise.</p>
<p><em>NOTE</em>: <code>Less</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-11">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>int64</code>, <code>bfloat16</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-10">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__matmul__">
<code><strong>matmul</strong></code>
</h3>
<pre class="python"><code>__matmul__(
    a,
    *args
)</code></pre>
<p>Multiplies matrix <code>a</code> by matrix <code>b</code>, producing <code>a</code> * <code>b</code>.</p>
<p>The inputs must, following any transpositions, be tensors of rank &gt;= 2 where the inner 2 dimensions specify valid matrix multiplication arguments, and any further outer dimensions match.</p>
<p>Both matrices must be of the same type. The supported types are: <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>complex64</code>, <code>complex128</code>.</p>
<p>Either matrix can be transposed or adjointed (conjugated and transposed) on the fly by setting one of the corresponding flag to <code>True</code>. These are <code>False</code> by default.</p>
<p>If one or both of the matrices contain a lot of zeros, a more efficient multiplication algorithm can be used by setting the corresponding <code>a_is_sparse</code> or <code>b_is_sparse</code> flag to <code>True</code>. These are <code>False</code> by default. This optimization is only available for plain matrices (rank-2 tensors) with datatypes <code>bfloat16</code> or <code>float32</code>.</p>
<p>For example:</p>
<pre class="python"><code># 2-D tensor `a`
# [[1, 2, 3],
#  [4, 5, 6]]
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

# 2-D tensor `b`
# [[ 7,  8],
#  [ 9, 10],
#  [11, 12]]
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])

# `a` * `b`
# [[ 58,  64],
#  [139, 154]]
c = tf.matmul(a, b)


# 3-D tensor `a`
# [[[ 1,  2,  3],
#   [ 4,  5,  6]],
#  [[ 7,  8,  9],
#   [10, 11, 12]]]
a = tf.constant(np.arange(1, 13, dtype=np.int32),
                shape=[2, 2, 3])

# 3-D tensor `b`
# [[[13, 14],
#   [15, 16],
#   [17, 18]],
#  [[19, 20],
#   [21, 22],
#   [23, 24]]]
b = tf.constant(np.arange(13, 25, dtype=np.int32),
                shape=[2, 3, 2])

# `a` * `b`
# [[[ 94, 100],
#   [229, 244]],
#  [[508, 532],
#   [697, 730]]]
c = tf.matmul(a, b)

# Since python &gt;= 3.5 the @ operator is supported (see PEP 465).
# In TensorFlow, it simply calls the `tf.matmul()` function, so the
# following lines are equivalent:
d = a @ b @ [[10.], [11.]]
d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])</code></pre>
<h4 id="args-12">Args:</h4>
<ul>
<li><b><code>a</code></b>: <code>Tensor</code> of type <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>complex64</code>, <code>complex128</code> and rank &gt; 1.</li>
<li><b><code>b</code></b>: <code>Tensor</code> with same type and rank as <code>a</code>.</li>
<li><b><code>transpose_a</code></b>: If <code>True</code>, <code>a</code> is transposed before multiplication.</li>
<li><b><code>transpose_b</code></b>: If <code>True</code>, <code>b</code> is transposed before multiplication.</li>
<li><b><code>adjoint_a</code></b>: If <code>True</code>, <code>a</code> is conjugated and transposed before multiplication.</li>
<li><b><code>adjoint_b</code></b>: If <code>True</code>, <code>b</code> is conjugated and transposed before multiplication.</li>
<li><b><code>a_is_sparse</code></b>: If <code>True</code>, <code>a</code> is treated as a sparse matrix.</li>
<li><b><code>b_is_sparse</code></b>: If <code>True</code>, <code>b</code> is treated as a sparse matrix.</li>
<li><b><code>name</code></b>: Name for the operation (optional).</li>
</ul>
<h4 id="returns-11">Returns:</h4>
<p>A <code>Tensor</code> of the same type as <code>a</code> and <code>b</code> where each inner-most matrix is the product of the corresponding matrices in <code>a</code> and <code>b</code>, e.g. if all transpose or adjoint attributes are <code>False</code>:</p>
<p><code>output</code>[…, i, j] = sum_k (<code>a</code>[…, i, k] * <code>b</code>[…, k, j]), for all indices i, j.</p>
<ul>
<li><b><code>Note</code></b>: This is matrix product, not element-wise product.</li>
</ul>
<h4 id="raises-2">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If transpose_a and adjoint_a, or transpose_b and adjoint_b are both set to True.</li>
</ul>
<h3 id="__mod__">
<code><strong>mod</strong></code>
</h3>
<pre class="python"><code>__mod__(
    a,
    *args
)</code></pre>
<p>Returns element-wise remainder of division. When <code>x &lt; 0</code> xor <code>y &lt; 0</code> is</p>
<p>true, this follows Python semantics in that the result here is consistent with a flooring divide. E.g. <code>floor(x / y) * y + mod(x, y) = x</code>.</p>
<p><em>NOTE</em>: <code>FloorMod</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-13">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>, <code>bfloat16</code>, <code>float32</code>, <code>float64</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-12">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__mul__">
<code><strong>mul</strong></code>
</h3>
<pre class="python"><code>__mul__(
    a,
    *args
)</code></pre>
<p>Dispatches cwise mul for “Dense*Dense&quot; and “Dense*Sparse“.</p>
<h3 id="__neg__">
<code><strong>neg</strong></code>
</h3>
<pre class="python"><code>__neg__(
    a,
    *args
)</code></pre>
<p>Computes numerical negative value element-wise.</p>
<p>I.e., \(y = -x\).</p>
<h4 id="args-14">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>bfloat16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-13">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__nonzero__">
<code><strong>nonzero</strong></code>
</h3>
<pre class="python"><code>__nonzero__()</code></pre>
<h3 id="__or__">
<code><strong>or</strong></code>
</h3>
<pre class="python"><code>__or__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of x OR y element-wise.</p>
<p><em>NOTE</em>: <code>LogicalOr</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-15">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-14">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__pow__">
<code><strong>pow</strong></code>
</h3>
<pre class="python"><code>__pow__(
    a,
    *args
)</code></pre>
<p>Computes the power of one value to another.</p>
<p>Given a tensor <code>x</code> and a tensor <code>y</code>, this operation computes \(x^y\) for corresponding elements in <code>x</code> and <code>y</code>. For example:</p>
<pre class="python"><code>x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
tf.pow(x, y)  # [[256, 65536], [9, 27]]</code></pre>
<h4 id="args-16">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-15">Returns:</h4>
<p>A <code>Tensor</code>.</p>
<h3 id="__radd__">
<code><strong>radd</strong></code>
</h3>
<pre class="python"><code>__radd__(
    a,
    *args
)</code></pre>
<p>Returns x + y element-wise.</p>
<p><em>NOTE</em>: <code>Add</code> supports broadcasting. <code>AddN</code> does not. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-17">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>bfloat16</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>, <code>string</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-16">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__rand__">
<code><strong>rand</strong></code>
</h3>
<pre class="python"><code>__rand__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of x AND y element-wise.</p>
<p><em>NOTE</em>: <code>LogicalAnd</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-18">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-17">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__rdiv__">
<code><strong>rdiv</strong></code>
</h3>
<pre class="python"><code>__rdiv__(
    a,
    *args
)</code></pre>
<p>Divide two values using Python 2 semantics. Used for Tensor.__div__.</p>
<h4 id="args-19">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li>
<li><b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-18">Returns:</h4>
<p><code>x / y</code> returns the quotient of x and y.</p>
<h3 id="__rfloordiv__">
<code><strong>rfloordiv</strong></code>
</h3>
<pre class="python"><code>__rfloordiv__(
    a,
    *args
)</code></pre>
<p>Divides <code>x / y</code> elementwise, rounding toward the most negative integer.</p>
<p>The same as <code>tf.div(x,y)</code> for integers, but uses <code>tf.floor(tf.div(x,y))</code> for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by <code>x // y</code> floor division in Python 3 and in Python 2.7 with <code>from __future__ import division</code>.</p>
<p>Note that for efficiency, <code>floordiv</code> uses C semantics for negative numbers (unlike Python and Numpy).</p>
<p><code>x</code> and <code>y</code> must have the same type, and the result will have the same type as well.</p>
<h4 id="args-20">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li>
<li><b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-19">Returns:</h4>
<p><code>x / y</code> rounded down (except possibly towards zero for negative integers).</p>
<h4 id="raises-3">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If the inputs are complex.</li>
</ul>
<h3 id="__rmatmul__">
<code><strong>rmatmul</strong></code>
</h3>
<pre class="python"><code>__rmatmul__(
    a,
    *args
)</code></pre>
<p>Multiplies matrix <code>a</code> by matrix <code>b</code>, producing <code>a</code> * <code>b</code>.</p>
<p>The inputs must, following any transpositions, be tensors of rank &gt;= 2 where the inner 2 dimensions specify valid matrix multiplication arguments, and any further outer dimensions match.</p>
<p>Both matrices must be of the same type. The supported types are: <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>complex64</code>, <code>complex128</code>.</p>
<p>Either matrix can be transposed or adjointed (conjugated and transposed) on the fly by setting one of the corresponding flag to <code>True</code>. These are <code>False</code> by default.</p>
<p>If one or both of the matrices contain a lot of zeros, a more efficient multiplication algorithm can be used by setting the corresponding <code>a_is_sparse</code> or <code>b_is_sparse</code> flag to <code>True</code>. These are <code>False</code> by default. This optimization is only available for plain matrices (rank-2 tensors) with datatypes <code>bfloat16</code> or <code>float32</code>.</p>
<p>For example:</p>
<pre class="python"><code># 2-D tensor `a`
# [[1, 2, 3],
#  [4, 5, 6]]
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

# 2-D tensor `b`
# [[ 7,  8],
#  [ 9, 10],
#  [11, 12]]
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])

# `a` * `b`
# [[ 58,  64],
#  [139, 154]]
c = tf.matmul(a, b)


# 3-D tensor `a`
# [[[ 1,  2,  3],
#   [ 4,  5,  6]],
#  [[ 7,  8,  9],
#   [10, 11, 12]]]
a = tf.constant(np.arange(1, 13, dtype=np.int32),
                shape=[2, 2, 3])

# 3-D tensor `b`
# [[[13, 14],
#   [15, 16],
#   [17, 18]],
#  [[19, 20],
#   [21, 22],
#   [23, 24]]]
b = tf.constant(np.arange(13, 25, dtype=np.int32),
                shape=[2, 3, 2])

# `a` * `b`
# [[[ 94, 100],
#   [229, 244]],
#  [[508, 532],
#   [697, 730]]]
c = tf.matmul(a, b)

# Since python &gt;= 3.5 the @ operator is supported (see PEP 465).
# In TensorFlow, it simply calls the `tf.matmul()` function, so the
# following lines are equivalent:
d = a @ b @ [[10.], [11.]]
d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])</code></pre>
<h4 id="args-21">Args:</h4>
<ul>
<li><b><code>a</code></b>: <code>Tensor</code> of type <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>complex64</code>, <code>complex128</code> and rank &gt; 1.</li>
<li><b><code>b</code></b>: <code>Tensor</code> with same type and rank as <code>a</code>.</li>
<li><b><code>transpose_a</code></b>: If <code>True</code>, <code>a</code> is transposed before multiplication.</li>
<li><b><code>transpose_b</code></b>: If <code>True</code>, <code>b</code> is transposed before multiplication.</li>
<li><b><code>adjoint_a</code></b>: If <code>True</code>, <code>a</code> is conjugated and transposed before multiplication.</li>
<li><b><code>adjoint_b</code></b>: If <code>True</code>, <code>b</code> is conjugated and transposed before multiplication.</li>
<li><b><code>a_is_sparse</code></b>: If <code>True</code>, <code>a</code> is treated as a sparse matrix.</li>
<li><b><code>b_is_sparse</code></b>: If <code>True</code>, <code>b</code> is treated as a sparse matrix.</li>
<li><b><code>name</code></b>: Name for the operation (optional).</li>
</ul>
<h4 id="returns-20">Returns:</h4>
<p>A <code>Tensor</code> of the same type as <code>a</code> and <code>b</code> where each inner-most matrix is the product of the corresponding matrices in <code>a</code> and <code>b</code>, e.g. if all transpose or adjoint attributes are <code>False</code>:</p>
<p><code>output</code>[…, i, j] = sum_k (<code>a</code>[…, i, k] * <code>b</code>[…, k, j]), for all indices i, j.</p>
<ul>
<li><b><code>Note</code></b>: This is matrix product, not element-wise product.</li>
</ul>
<h4 id="raises-4">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If transpose_a and adjoint_a, or transpose_b and adjoint_b are both set to True.</li>
</ul>
<h3 id="__rmod__">
<code><strong>rmod</strong></code>
</h3>
<pre class="python"><code>__rmod__(
    a,
    *args
)</code></pre>
<p>Returns element-wise remainder of division. When <code>x &lt; 0</code> xor <code>y &lt; 0</code> is</p>
<p>true, this follows Python semantics in that the result here is consistent with a flooring divide. E.g. <code>floor(x / y) * y + mod(x, y) = x</code>.</p>
<p><em>NOTE</em>: <code>FloorMod</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-22">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>, <code>bfloat16</code>, <code>float32</code>, <code>float64</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-21">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__rmul__">
<code><strong>rmul</strong></code>
</h3>
<pre class="python"><code>__rmul__(
    a,
    *args
)</code></pre>
<p>Dispatches cwise mul for “Dense*Dense&quot; and “Dense*Sparse“.</p>
<h3 id="__ror__">
<code><strong>ror</strong></code>
</h3>
<pre class="python"><code>__ror__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of x OR y element-wise.</p>
<p><em>NOTE</em>: <code>LogicalOr</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-23">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-22">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__rpow__">
<code><strong>rpow</strong></code>
</h3>
<pre class="python"><code>__rpow__(
    a,
    *args
)</code></pre>
<p>Computes the power of one value to another.</p>
<p>Given a tensor <code>x</code> and a tensor <code>y</code>, this operation computes \(x^y\) for corresponding elements in <code>x</code> and <code>y</code>. For example:</p>
<pre class="python"><code>x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
tf.pow(x, y)  # [[256, 65536], [9, 27]]</code></pre>
<h4 id="args-24">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-23">Returns:</h4>
<p>A <code>Tensor</code>.</p>
<h3 id="__rsub__">
<code><strong>rsub</strong></code>
</h3>
<pre class="python"><code>__rsub__(
    a,
    *args
)</code></pre>
<p>Returns x - y element-wise.</p>
<p><em>NOTE</em>: <code>Subtract</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-25">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>bfloat16</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>uint16</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-24">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__rtruediv__">
<code><strong>rtruediv</strong></code>
</h3>
<pre class="python"><code>__rtruediv__(
    a,
    *args
)</code></pre>
<h3 id="__rxor__">
<code><strong>rxor</strong></code>
</h3>
<pre class="python"><code>__rxor__(
    a,
    *args
)</code></pre>
<p>x ^ y = (x | y) &amp; ~(x &amp; y).</p>
<h3 id="__sub__">
<code><strong>sub</strong></code>
</h3>
<pre class="python"><code>__sub__(
    a,
    *args
)</code></pre>
<p>Returns x - y element-wise.</p>
<p><em>NOTE</em>: <code>Subtract</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-26">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>bfloat16</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>uint16</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-25">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__truediv__">
<code><strong>truediv</strong></code>
</h3>
<pre class="python"><code>__truediv__(
    a,
    *args
)</code></pre>
<h3 id="__xor__">
<code><strong>xor</strong></code>
</h3>
<pre class="python"><code>__xor__(
    a,
    *args
)</code></pre>
<p>x ^ y = (x | y) &amp; ~(x &amp; y).</p>
<h3 id="eval">
<code>eval</code>
</h3>
<pre class="python"><code>eval(
    session=None,
    feed_dict=None
)</code></pre>
<p>In a session, computes and returns the value of this random variable.</p>
<p>This is not a graph construction method, it does not add ops to the graph.</p>
<p>This convenience method requires a session where the graph containing this variable has been launched. If no session is passed, the default session is used.</p>
<h4 id="args-27">Args:</h4>
<ul>
<li><b><code>session</code></b>: tf.BaseSession. The <code>tf.Session</code> to use to evaluate this random variable. If none, the default session is used.</li>
<li><b><code>feed_dict</code></b>: dict. A dictionary that maps <code>tf.Tensor</code> objects to feed values. See <code>tf.Session.run()</code> for a description of the valid feed values.</li>
</ul>
<h4 id="examples-1">Examples</h4>
<pre class="python"><code>x = Normal(0.0, 1.0)
with tf.Session() as sess:
  # Usage passing the session explicitly.
  print(x.eval(sess))
  # Usage with the default session.  The &#39;with&#39; block
  # above makes &#39;sess&#39; the default session.
  print(x.eval())</code></pre>
<h3 id="get_ancestors">
<code>get_ancestors</code>
</h3>
<pre class="python"><code>get_ancestors(collection=None)</code></pre>
<p>Get ancestor random variables.</p>
<h3 id="get_blanket">
<code>get_blanket</code>
</h3>
<pre class="python"><code>get_blanket(collection=None)</code></pre>
<p>Get the random variable’s Markov blanket.</p>
<h3 id="get_children">
<code>get_children</code>
</h3>
<pre class="python"><code>get_children(collection=None)</code></pre>
<p>Get child random variables.</p>
<h3 id="get_descendants">
<code>get_descendants</code>
</h3>
<pre class="python"><code>get_descendants(collection=None)</code></pre>
<p>Get descendant random variables.</p>
<h3 id="get_parents">
<code>get_parents</code>
</h3>
<pre class="python"><code>get_parents(collection=None)</code></pre>
<p>Get parent random variables.</p>
<h3 id="get_shape">
<code>get_shape</code>
</h3>
<pre class="python"><code>get_shape()</code></pre>
<p>Get shape of random variable.</p>
<h3 id="get_siblings">
<code>get_siblings</code>
</h3>
<pre class="python"><code>get_siblings(collection=None)</code></pre>
<p>Get sibling random variables.</p>
<h3 id="get_variables">
<code>get_variables</code>
</h3>
<pre class="python"><code>get_variables(collection=None)</code></pre>
<p>Get TensorFlow variables that the random variable depends on.</p>
<h3 id="value">
<code>value</code>
</h3>
<pre class="python"><code>value()</code></pre>
<p>Get tensor that the random variable corresponds to.</p>
<h2 id="class-members">Class Members</h2>
<h3 id="__array_priority__">
<code><strong>array_priority</strong></code>
</h3>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
