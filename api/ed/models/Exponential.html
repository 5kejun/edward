<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward – ed.models.Exponential</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  /* Downgrade API's header styles without explicitly modifying their type. */
  div.nine.columns h1 { text-align: left; }
  div.nine.columns h1 { font-size: 3.0rem; line-height: 1.25; letter-spacing: -.1rem; }
  div.nine.columns h2 { font-size: 2.4rem; line-height: 1.3;  letter-spacing: -.1rem; }
  div.nine.columns h3 { font-size: 2.4rem; line-height: 1.35; letter-spacing: -.08rem; }
  div.nine.columns h4 { font-size: 1.8rem; line-height: 1.5;  letter-spacing: -.05rem; }
  div.nine.columns h5 { font-size: 1.5rem; line-height: 1.6;  letter-spacing: 0; }
  @media (min-width: 550px) {
    div.nine.columns h1 { font-size: 3.6rem; }
    div.nine.columns h2 { font-size: 3.0rem; }
    div.nine.columns h3 { font-size: 3.0rem; }
    div.nine.columns h4 { font-size: 2.5rem; }
    div.nine.columns h5 { font-size: 1.5rem; }
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/api/">API</a>
    <hr style="margin-top: 1rem; margin-bottom: 1.5rem;"/>
    <a class="button u-full-width" href="/api/reference">Reference</a>
    <a class="button u-full-width" href="/api/ed/criticisms">ed.criticisms</a>
<a class="button u-full-width" href="/api/ed/inferences">ed.inferences</a>
<a class="button u-full-width" href="/api/ed/models">ed.models</a>
<a class="button u-full-width" href="/api/ed/util">ed.util</a>
<a class="button u-full-width" href="/api/observations">observations</a>

    <div class="row" style="padding-bottom: 5%"> </div>
    <a class="button2 u-pull-right" style="padding-right:10%"
      href="https://github.com/blei-lab/edward">
      <span style="vertical-align:middle;">Github</span>&nbsp;
      <img src="/images/github-mark.svg" style="vertical-align:middle;"
      alt="Edward on Github" />
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">

<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="ed.models.Exponential" />
<meta itemprop="property" content="allow_nan_stats"/>
<meta itemprop="property" content="batch_shape"/>
<meta itemprop="property" content="concentration"/>
<meta itemprop="property" content="dtype"/>
<meta itemprop="property" content="event_shape"/>
<meta itemprop="property" content="name"/>
<meta itemprop="property" content="parameters"/>
<meta itemprop="property" content="rate"/>
<meta itemprop="property" content="reparameterization_type"/>
<meta itemprop="property" content="sample_shape"/>
<meta itemprop="property" content="shape"/>
<meta itemprop="property" content="validate_args"/>
<meta itemprop="property" content="__abs__"/>
<meta itemprop="property" content="__add__"/>
<meta itemprop="property" content="__and__"/>
<meta itemprop="property" content="__bool__"/>
<meta itemprop="property" content="__div__"/>
<meta itemprop="property" content="__eq__"/>
<meta itemprop="property" content="__floordiv__"/>
<meta itemprop="property" content="__ge__"/>
<meta itemprop="property" content="__getitem__"/>
<meta itemprop="property" content="__gt__"/>
<meta itemprop="property" content="__init__"/>
<meta itemprop="property" content="__invert__"/>
<meta itemprop="property" content="__iter__"/>
<meta itemprop="property" content="__le__"/>
<meta itemprop="property" content="__lt__"/>
<meta itemprop="property" content="__matmul__"/>
<meta itemprop="property" content="__mod__"/>
<meta itemprop="property" content="__mul__"/>
<meta itemprop="property" content="__neg__"/>
<meta itemprop="property" content="__nonzero__"/>
<meta itemprop="property" content="__or__"/>
<meta itemprop="property" content="__pow__"/>
<meta itemprop="property" content="__radd__"/>
<meta itemprop="property" content="__rand__"/>
<meta itemprop="property" content="__rdiv__"/>
<meta itemprop="property" content="__rfloordiv__"/>
<meta itemprop="property" content="__rmatmul__"/>
<meta itemprop="property" content="__rmod__"/>
<meta itemprop="property" content="__rmul__"/>
<meta itemprop="property" content="__ror__"/>
<meta itemprop="property" content="__rpow__"/>
<meta itemprop="property" content="__rsub__"/>
<meta itemprop="property" content="__rtruediv__"/>
<meta itemprop="property" content="__rxor__"/>
<meta itemprop="property" content="__sub__"/>
<meta itemprop="property" content="__truediv__"/>
<meta itemprop="property" content="__xor__"/>
<meta itemprop="property" content="batch_shape_tensor"/>
<meta itemprop="property" content="cdf"/>
<meta itemprop="property" content="conjugate_log_prob"/>
<meta itemprop="property" content="copy"/>
<meta itemprop="property" content="covariance"/>
<meta itemprop="property" content="cross_entropy"/>
<meta itemprop="property" content="entropy"/>
<meta itemprop="property" content="eval"/>
<meta itemprop="property" content="event_shape_tensor"/>
<meta itemprop="property" content="get_ancestors"/>
<meta itemprop="property" content="get_blanket"/>
<meta itemprop="property" content="get_children"/>
<meta itemprop="property" content="get_descendants"/>
<meta itemprop="property" content="get_parents"/>
<meta itemprop="property" content="get_shape"/>
<meta itemprop="property" content="get_siblings"/>
<meta itemprop="property" content="get_variables"/>
<meta itemprop="property" content="is_scalar_batch"/>
<meta itemprop="property" content="is_scalar_event"/>
<meta itemprop="property" content="kl_divergence"/>
<meta itemprop="property" content="log_cdf"/>
<meta itemprop="property" content="log_prob"/>
<meta itemprop="property" content="log_survival_function"/>
<meta itemprop="property" content="mean"/>
<meta itemprop="property" content="mode"/>
<meta itemprop="property" content="param_shapes"/>
<meta itemprop="property" content="param_static_shapes"/>
<meta itemprop="property" content="prob"/>
<meta itemprop="property" content="quantile"/>
<meta itemprop="property" content="sample"/>
<meta itemprop="property" content="stddev"/>
<meta itemprop="property" content="survival_function"/>
<meta itemprop="property" content="value"/>
<meta itemprop="property" content="variance"/>
<meta itemprop="property" content="__array_priority__"/>
<meta itemprop="property" content="support"/>
</div>
<h1 id="ed.models.exponential">ed.models.Exponential</h1>
<h2 id="class-exponential">Class <code>Exponential</code></h2>
<p>Inherits From: <a href="../../ed/RandomVariable"><code>RandomVariable</code></a></p>
<p>Exponential distribution.</p>
<p>The Exponential distribution is parameterized by an event <code>rate</code> parameter.</p>
<h4 id="mathematical-details">Mathematical Details</h4>
<p>The probability density function (pdf) is,</p>
<pre class="none"><code>pdf(x; lambda, x &gt; 0) = exp(-lambda x) / Z
Z = 1 / lambda</code></pre>
<p>where <code>rate = lambda</code> and <code>Z</code> is the normalizaing constant.</p>
<p>The Exponential distribution is a special case of the Gamma distribution, i.e.,</p>
<pre class="python"><code>Exponential(rate) = Gamma(concentration=1., rate)</code></pre>
<p>The Exponential distribution uses a <code>rate</code> parameter, or “inverse scale”, which can be intuited as,</p>
<pre class="none"><code>X ~ Exponential(rate=1)
Y = X / rate</code></pre>
<h2 id="properties">Properties</h2>
<h3 id="allow_nan_stats">
<code>allow_nan_stats</code>
</h3>
<p>Python <code>bool</code> describing behavior when a stat is undefined.</p>
<p>Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution’s pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student’s T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined.</p>
<h4 id="returns">Returns:</h4>
<ul>
<li><b><code>allow_nan_stats</code></b>: Python <code>bool</code>.</li>
</ul>
<h3 id="batch_shape">
<code>batch_shape</code>
</h3>
<p>Shape of a single sample from a single event index as a <code>TensorShape</code>.</p>
<p>May be partially defined or unknown.</p>
<p>The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.</p>
<h4 id="returns-1">Returns:</h4>
<ul>
<li><b><code>batch_shape</code></b>: <code>TensorShape</code>, possibly unknown.</li>
</ul>
<h3 id="concentration">
<code>concentration</code>
</h3>
<p>Concentration parameter.</p>
<h3 id="dtype">
<code>dtype</code>
</h3>
<p>The <code>DType</code> of <code>Tensor</code>s handled by this <code>Distribution</code>.</p>
<h3 id="event_shape">
<code>event_shape</code>
</h3>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>May be partially defined or unknown.</p>
<h4 id="returns-2">Returns:</h4>
<ul>
<li><b><code>event_shape</code></b>: <code>TensorShape</code>, possibly unknown.</li>
</ul>
<h3 id="name">
<code>name</code>
</h3>
<p>Name prepended to all ops created by this <code>Distribution</code>.</p>
<h3 id="parameters">
<code>parameters</code>
</h3>
<p>Dictionary of parameters used to instantiate this <code>Distribution</code>.</p>
<h3 id="rate">
<code>rate</code>
</h3>
<h3 id="reparameterization_type">
<code>reparameterization_type</code>
</h3>
<p>Describes how samples from the distribution are reparameterized.</p>
<p>Currently this is one of the static instances <code>distributions.FULLY_REPARAMETERIZED</code> or <code>distributions.NOT_REPARAMETERIZED</code>.</p>
<h4 id="returns-3">Returns:</h4>
<p>An instance of <code>ReparameterizationType</code>.</p>
<h3 id="sample_shape">
<code>sample_shape</code>
</h3>
<p>Sample shape of random variable.</p>
<h3 id="shape">
<code>shape</code>
</h3>
<p>Shape of random variable.</p>
<h3 id="validate_args">
<code>validate_args</code>
</h3>
<p>Python <code>bool</code> indicating possibly expensive checks are enabled.</p>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<pre class="python"><code>__init__(
    *args,
    **kwargs
)</code></pre>
<p>Construct Exponential distribution with parameter <code>rate</code>.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>rate</code></b>: Floating point tensor, equivalent to <code>1 / mean</code>. Must contain only positive values.</li>
<li><b><code>validate_args</code></b>: Python <code>bool</code>, default <code>False</code>. When <code>True</code> distribution parameters are checked for validity despite possibly degrading runtime performance. When <code>False</code> invalid inputs may silently render incorrect outputs.</li>
<li><b><code>allow_nan_stats</code></b>: Python <code>bool</code>, default <code>True</code>. When <code>True</code>, statistics (e.g., mean, mode, variance) use the value “<code>NaN</code>” to indicate the result is undefined. When <code>False</code>, an exception is raised if one or more of the statistic’s batch members are undefined.</li>
<li><b><code>name</code></b>: Python <code>str</code> name prefixed to Ops created by this class.</li>
</ul>
<h3 id="__abs__">
<code><strong>abs</strong></code>
</h3>
<pre class="python"><code>__abs__(
    a,
    *args
)</code></pre>
<p>Computes the absolute value of a tensor.</p>
<p>Given a tensor <code>x</code> of complex numbers, this operation returns a tensor of type <code>float32</code> or <code>float64</code> that is the absolute value of each element in <code>x</code>. All elements in <code>x</code> must be complex numbers of the form \(a + bj\). The absolute value is computed as \( \). For example:</p>
<pre class="python"><code>x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])
tf.abs(x)  # [5.25594902, 6.60492229]</code></pre>
<h4 id="args-1">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> or <code>SparseTensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code> or <code>complex128</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-4">Returns:</h4>
<p>A <code>Tensor</code> or <code>SparseTensor</code> the same size and type as <code>x</code> with absolute values. Note, for <code>complex64</code> or <code>complex128</code> input, the returned <code>Tensor</code> will be of type <code>float32</code> or <code>float64</code>, respectively.</p>
<h3 id="__add__">
<code><strong>add</strong></code>
</h3>
<pre class="python"><code>__add__(
    a,
    *args
)</code></pre>
<p>Returns x + y element-wise.</p>
<p><em>NOTE</em>: <code>Add</code> supports broadcasting. <code>AddN</code> does not. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-2">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>bfloat16</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>, <code>string</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-5">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__and__">
<code><strong>and</strong></code>
</h3>
<pre class="python"><code>__and__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of x AND y element-wise.</p>
<p><em>NOTE</em>: <code>LogicalAnd</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-3">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-6">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__bool__">
<code><strong>bool</strong></code>
</h3>
<pre class="python"><code>__bool__()</code></pre>
<h3 id="__div__">
<code><strong>div</strong></code>
</h3>
<pre class="python"><code>__div__(
    a,
    *args
)</code></pre>
<p>Divide two values using Python 2 semantics. Used for Tensor.__div__.</p>
<h4 id="args-4">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li>
<li><b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-7">Returns:</h4>
<p><code>x / y</code> returns the quotient of x and y.</p>
<h3 id="__eq__">
<code><strong>eq</strong></code>
</h3>
<pre class="python"><code>__eq__(other)</code></pre>
<h3 id="__floordiv__">
<code><strong>floordiv</strong></code>
</h3>
<pre class="python"><code>__floordiv__(
    a,
    *args
)</code></pre>
<p>Divides <code>x / y</code> elementwise, rounding toward the most negative integer.</p>
<p>The same as <code>tf.div(x,y)</code> for integers, but uses <code>tf.floor(tf.div(x,y))</code> for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by <code>x // y</code> floor division in Python 3 and in Python 2.7 with <code>from __future__ import division</code>.</p>
<p>Note that for efficiency, <code>floordiv</code> uses C semantics for negative numbers (unlike Python and Numpy).</p>
<p><code>x</code> and <code>y</code> must have the same type, and the result will have the same type as well.</p>
<h4 id="args-5">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li>
<li><b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-8">Returns:</h4>
<p><code>x / y</code> rounded down (except possibly towards zero for negative integers).</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If the inputs are complex.</li>
</ul>
<h3 id="__ge__">
<code><strong>ge</strong></code>
</h3>
<pre class="python"><code>__ge__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of (x &gt;= y) element-wise.</p>
<p><em>NOTE</em>: <code>GreaterEqual</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-6">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>, <code>bfloat16</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-9">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__getitem__">
<code><strong>getitem</strong></code>
</h3>
<pre class="python"><code>__getitem__(
    a,
    *args
)</code></pre>
<p>Overload for Tensor.__getitem__.</p>
<p>This operation extracts the specified region from the tensor. The notation is similar to NumPy with the restriction that currently only support basic indexing. That means that using a tensor as input is not currently allowed</p>
<p>Some useful examples:</p>
<pre class="python"><code># strip leading and trailing 2 elements
foo = tf.constant([1,2,3,4,5,6])
print(foo[2:-2].eval())  # [3,4]

# skip every row and reverse every column
foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])
print(foo[::2,::-1].eval())  # [[3,2,1], [9,8,7]]

# Insert another dimension
foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])
print(foo[tf.newaxis, :, :].eval()) # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]
print(foo[:, tf.newaxis, :].eval()) # =&gt; [[[1,2,3]], [[4,5,6]], [[7,8,9]]]
print(foo[:, :, tf.newaxis].eval()) # =&gt; [[[1],[2],[3]], [[4],[5],[6]],
[[7],[8],[9]]]

# Ellipses (3 equivalent operations)
foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])
print(foo[tf.newaxis, :, :].eval())  # [[[1,2,3], [4,5,6], [7,8,9]]]
print(foo[tf.newaxis, ...].eval())  # [[[1,2,3], [4,5,6], [7,8,9]]]
print(foo[tf.newaxis].eval())  # [[[1,2,3], [4,5,6], [7,8,9]]]</code></pre>
<p>Notes: - <code>tf.newaxis</code> is <code>None</code> as in NumPy. - An implicit ellipsis is placed at the end of the <code>slice_spec</code> - NumPy advanced indexing is currently not supported.</p>
<h4 id="args-7">Args:</h4>
<ul>
<li><b><code>tensor</code></b>: An ops.Tensor object.</li>
<li><b><code>slice_spec</code></b>: The arguments to Tensor.__getitem__.</li>
<li><b><code>var</code></b>: In the case of variable slice assignment, the Variable object to slice (i.e. tensor is the read-only view of this variable).</li>
</ul>
<h4 id="returns-10">Returns:</h4>
<p>The appropriate slice of “tensor”, based on “slice_spec”.</p>
<h4 id="raises-1">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If a slice range is negative size.</li>
<li><b><code>TypeError</code></b>: If the slice indices aren’t int, slice, or Ellipsis.</li>
</ul>
<h3 id="__gt__">
<code><strong>gt</strong></code>
</h3>
<pre class="python"><code>__gt__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of (x &gt; y) element-wise.</p>
<p><em>NOTE</em>: <code>Greater</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-8">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>, <code>bfloat16</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-11">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__invert__">
<code><strong>invert</strong></code>
</h3>
<pre class="python"><code>__invert__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of NOT x element-wise.</p>
<h4 id="args-9">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-12">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__iter__">
<code><strong>iter</strong></code>
</h3>
<pre class="python"><code>__iter__()</code></pre>
<h3 id="__le__">
<code><strong>le</strong></code>
</h3>
<pre class="python"><code>__le__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of (x &lt;= y) element-wise.</p>
<p><em>NOTE</em>: <code>LessEqual</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-10">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>, <code>bfloat16</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-13">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__lt__">
<code><strong>lt</strong></code>
</h3>
<pre class="python"><code>__lt__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of (x &lt; y) element-wise.</p>
<p><em>NOTE</em>: <code>Less</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-11">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>, <code>bfloat16</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-14">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__matmul__">
<code><strong>matmul</strong></code>
</h3>
<pre class="python"><code>__matmul__(
    a,
    *args
)</code></pre>
<p>Multiplies matrix <code>a</code> by matrix <code>b</code>, producing <code>a</code> * <code>b</code>.</p>
<p>The inputs must, following any transpositions, be tensors of rank &gt;= 2 where the inner 2 dimensions specify valid matrix multiplication arguments, and any further outer dimensions match.</p>
<p>Both matrices must be of the same type. The supported types are: <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>complex64</code>, <code>complex128</code>.</p>
<p>Either matrix can be transposed or adjointed (conjugated and transposed) on the fly by setting one of the corresponding flag to <code>True</code>. These are <code>False</code> by default.</p>
<p>If one or both of the matrices contain a lot of zeros, a more efficient multiplication algorithm can be used by setting the corresponding <code>a_is_sparse</code> or <code>b_is_sparse</code> flag to <code>True</code>. These are <code>False</code> by default. This optimization is only available for plain matrices (rank-2 tensors) with datatypes <code>bfloat16</code> or <code>float32</code>.</p>
<p>For example:</p>
<pre class="python"><code># 2-D tensor `a`
# [[1, 2, 3],
#  [4, 5, 6]]
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

# 2-D tensor `b`
# [[ 7,  8],
#  [ 9, 10],
#  [11, 12]]
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])

# `a` * `b`
# [[ 58,  64],
#  [139, 154]]
c = tf.matmul(a, b)


# 3-D tensor `a`
# [[[ 1,  2,  3],
#   [ 4,  5,  6]],
#  [[ 7,  8,  9],
#   [10, 11, 12]]]
a = tf.constant(np.arange(1, 13, dtype=np.int32),
                shape=[2, 2, 3])

# 3-D tensor `b`
# [[[13, 14],
#   [15, 16],
#   [17, 18]],
#  [[19, 20],
#   [21, 22],
#   [23, 24]]]
b = tf.constant(np.arange(13, 25, dtype=np.int32),
                shape=[2, 3, 2])

# `a` * `b`
# [[[ 94, 100],
#   [229, 244]],
#  [[508, 532],
#   [697, 730]]]
c = tf.matmul(a, b)

# Since python &gt;= 3.5 the @ operator is supported (see PEP 465).
# In TensorFlow, it simply calls the `tf.matmul()` function, so the
# following lines are equivalent:
d = a @ b @ [[10.], [11.]]
d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])</code></pre>
<h4 id="args-12">Args:</h4>
<ul>
<li><b><code>a</code></b>: <code>Tensor</code> of type <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>complex64</code>, <code>complex128</code> and rank &gt; 1.</li>
<li><b><code>b</code></b>: <code>Tensor</code> with same type and rank as <code>a</code>.</li>
<li><b><code>transpose_a</code></b>: If <code>True</code>, <code>a</code> is transposed before multiplication.</li>
<li><b><code>transpose_b</code></b>: If <code>True</code>, <code>b</code> is transposed before multiplication.</li>
<li><b><code>adjoint_a</code></b>: If <code>True</code>, <code>a</code> is conjugated and transposed before multiplication.</li>
<li><b><code>adjoint_b</code></b>: If <code>True</code>, <code>b</code> is conjugated and transposed before multiplication.</li>
<li><b><code>a_is_sparse</code></b>: If <code>True</code>, <code>a</code> is treated as a sparse matrix.</li>
<li><b><code>b_is_sparse</code></b>: If <code>True</code>, <code>b</code> is treated as a sparse matrix.</li>
<li><b><code>name</code></b>: Name for the operation (optional).</li>
</ul>
<h4 id="returns-15">Returns:</h4>
<p>A <code>Tensor</code> of the same type as <code>a</code> and <code>b</code> where each inner-most matrix is the product of the corresponding matrices in <code>a</code> and <code>b</code>, e.g. if all transpose or adjoint attributes are <code>False</code>:</p>
<p><code>output</code>[…, i, j] = sum_k (<code>a</code>[…, i, k] * <code>b</code>[…, k, j]), for all indices i, j.</p>
<ul>
<li><b><code>Note</code></b>: This is matrix product, not element-wise product.</li>
</ul>
<h4 id="raises-2">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If transpose_a and adjoint_a, or transpose_b and adjoint_b are both set to True.</li>
</ul>
<h3 id="__mod__">
<code><strong>mod</strong></code>
</h3>
<pre class="python"><code>__mod__(
    a,
    *args
)</code></pre>
<p>Returns element-wise remainder of division. When <code>x &lt; 0</code> xor <code>y &lt; 0</code> is</p>
<p>true, this follows Python semantics in that the result here is consistent with a flooring divide. E.g. <code>floor(x / y) * y + mod(x, y) = x</code>.</p>
<p><em>NOTE</em>: <code>FloorMod</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-13">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>, <code>bfloat16</code>, <code>float32</code>, <code>float64</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-16">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__mul__">
<code><strong>mul</strong></code>
</h3>
<pre class="python"><code>__mul__(
    a,
    *args
)</code></pre>
<p>Dispatches cwise mul for “Dense*Dense&quot; and “Dense*Sparse“.</p>
<h3 id="__neg__">
<code><strong>neg</strong></code>
</h3>
<pre class="python"><code>__neg__(
    a,
    *args
)</code></pre>
<p>Computes numerical negative value element-wise.</p>
<p>I.e., \(y = -x\).</p>
<h4 id="args-14">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>bfloat16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-17">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__nonzero__">
<code><strong>nonzero</strong></code>
</h3>
<pre class="python"><code>__nonzero__()</code></pre>
<h3 id="__or__">
<code><strong>or</strong></code>
</h3>
<pre class="python"><code>__or__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of x OR y element-wise.</p>
<p><em>NOTE</em>: <code>LogicalOr</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-15">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-18">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__pow__">
<code><strong>pow</strong></code>
</h3>
<pre class="python"><code>__pow__(
    a,
    *args
)</code></pre>
<p>Computes the power of one value to another.</p>
<p>Given a tensor <code>x</code> and a tensor <code>y</code>, this operation computes \(x^y\) for corresponding elements in <code>x</code> and <code>y</code>. For example:</p>
<pre class="python"><code>x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
tf.pow(x, y)  # [[256, 65536], [9, 27]]</code></pre>
<h4 id="args-16">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-19">Returns:</h4>
<p>A <code>Tensor</code>.</p>
<h3 id="__radd__">
<code><strong>radd</strong></code>
</h3>
<pre class="python"><code>__radd__(
    a,
    *args
)</code></pre>
<p>Returns x + y element-wise.</p>
<p><em>NOTE</em>: <code>Add</code> supports broadcasting. <code>AddN</code> does not. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-17">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>bfloat16</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>, <code>string</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-20">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__rand__">
<code><strong>rand</strong></code>
</h3>
<pre class="python"><code>__rand__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of x AND y element-wise.</p>
<p><em>NOTE</em>: <code>LogicalAnd</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-18">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-21">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__rdiv__">
<code><strong>rdiv</strong></code>
</h3>
<pre class="python"><code>__rdiv__(
    a,
    *args
)</code></pre>
<p>Divide two values using Python 2 semantics. Used for Tensor.__div__.</p>
<h4 id="args-19">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li>
<li><b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-22">Returns:</h4>
<p><code>x / y</code> returns the quotient of x and y.</p>
<h3 id="__rfloordiv__">
<code><strong>rfloordiv</strong></code>
</h3>
<pre class="python"><code>__rfloordiv__(
    a,
    *args
)</code></pre>
<p>Divides <code>x / y</code> elementwise, rounding toward the most negative integer.</p>
<p>The same as <code>tf.div(x,y)</code> for integers, but uses <code>tf.floor(tf.div(x,y))</code> for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by <code>x // y</code> floor division in Python 3 and in Python 2.7 with <code>from __future__ import division</code>.</p>
<p>Note that for efficiency, <code>floordiv</code> uses C semantics for negative numbers (unlike Python and Numpy).</p>
<p><code>x</code> and <code>y</code> must have the same type, and the result will have the same type as well.</p>
<h4 id="args-20">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li>
<li><b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-23">Returns:</h4>
<p><code>x / y</code> rounded down (except possibly towards zero for negative integers).</p>
<h4 id="raises-3">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If the inputs are complex.</li>
</ul>
<h3 id="__rmatmul__">
<code><strong>rmatmul</strong></code>
</h3>
<pre class="python"><code>__rmatmul__(
    a,
    *args
)</code></pre>
<p>Multiplies matrix <code>a</code> by matrix <code>b</code>, producing <code>a</code> * <code>b</code>.</p>
<p>The inputs must, following any transpositions, be tensors of rank &gt;= 2 where the inner 2 dimensions specify valid matrix multiplication arguments, and any further outer dimensions match.</p>
<p>Both matrices must be of the same type. The supported types are: <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>complex64</code>, <code>complex128</code>.</p>
<p>Either matrix can be transposed or adjointed (conjugated and transposed) on the fly by setting one of the corresponding flag to <code>True</code>. These are <code>False</code> by default.</p>
<p>If one or both of the matrices contain a lot of zeros, a more efficient multiplication algorithm can be used by setting the corresponding <code>a_is_sparse</code> or <code>b_is_sparse</code> flag to <code>True</code>. These are <code>False</code> by default. This optimization is only available for plain matrices (rank-2 tensors) with datatypes <code>bfloat16</code> or <code>float32</code>.</p>
<p>For example:</p>
<pre class="python"><code># 2-D tensor `a`
# [[1, 2, 3],
#  [4, 5, 6]]
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

# 2-D tensor `b`
# [[ 7,  8],
#  [ 9, 10],
#  [11, 12]]
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])

# `a` * `b`
# [[ 58,  64],
#  [139, 154]]
c = tf.matmul(a, b)


# 3-D tensor `a`
# [[[ 1,  2,  3],
#   [ 4,  5,  6]],
#  [[ 7,  8,  9],
#   [10, 11, 12]]]
a = tf.constant(np.arange(1, 13, dtype=np.int32),
                shape=[2, 2, 3])

# 3-D tensor `b`
# [[[13, 14],
#   [15, 16],
#   [17, 18]],
#  [[19, 20],
#   [21, 22],
#   [23, 24]]]
b = tf.constant(np.arange(13, 25, dtype=np.int32),
                shape=[2, 3, 2])

# `a` * `b`
# [[[ 94, 100],
#   [229, 244]],
#  [[508, 532],
#   [697, 730]]]
c = tf.matmul(a, b)

# Since python &gt;= 3.5 the @ operator is supported (see PEP 465).
# In TensorFlow, it simply calls the `tf.matmul()` function, so the
# following lines are equivalent:
d = a @ b @ [[10.], [11.]]
d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])</code></pre>
<h4 id="args-21">Args:</h4>
<ul>
<li><b><code>a</code></b>: <code>Tensor</code> of type <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>complex64</code>, <code>complex128</code> and rank &gt; 1.</li>
<li><b><code>b</code></b>: <code>Tensor</code> with same type and rank as <code>a</code>.</li>
<li><b><code>transpose_a</code></b>: If <code>True</code>, <code>a</code> is transposed before multiplication.</li>
<li><b><code>transpose_b</code></b>: If <code>True</code>, <code>b</code> is transposed before multiplication.</li>
<li><b><code>adjoint_a</code></b>: If <code>True</code>, <code>a</code> is conjugated and transposed before multiplication.</li>
<li><b><code>adjoint_b</code></b>: If <code>True</code>, <code>b</code> is conjugated and transposed before multiplication.</li>
<li><b><code>a_is_sparse</code></b>: If <code>True</code>, <code>a</code> is treated as a sparse matrix.</li>
<li><b><code>b_is_sparse</code></b>: If <code>True</code>, <code>b</code> is treated as a sparse matrix.</li>
<li><b><code>name</code></b>: Name for the operation (optional).</li>
</ul>
<h4 id="returns-24">Returns:</h4>
<p>A <code>Tensor</code> of the same type as <code>a</code> and <code>b</code> where each inner-most matrix is the product of the corresponding matrices in <code>a</code> and <code>b</code>, e.g. if all transpose or adjoint attributes are <code>False</code>:</p>
<p><code>output</code>[…, i, j] = sum_k (<code>a</code>[…, i, k] * <code>b</code>[…, k, j]), for all indices i, j.</p>
<ul>
<li><b><code>Note</code></b>: This is matrix product, not element-wise product.</li>
</ul>
<h4 id="raises-4">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If transpose_a and adjoint_a, or transpose_b and adjoint_b are both set to True.</li>
</ul>
<h3 id="__rmod__">
<code><strong>rmod</strong></code>
</h3>
<pre class="python"><code>__rmod__(
    a,
    *args
)</code></pre>
<p>Returns element-wise remainder of division. When <code>x &lt; 0</code> xor <code>y &lt; 0</code> is</p>
<p>true, this follows Python semantics in that the result here is consistent with a flooring divide. E.g. <code>floor(x / y) * y + mod(x, y) = x</code>.</p>
<p><em>NOTE</em>: <code>FloorMod</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-22">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>, <code>bfloat16</code>, <code>float32</code>, <code>float64</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-25">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__rmul__">
<code><strong>rmul</strong></code>
</h3>
<pre class="python"><code>__rmul__(
    a,
    *args
)</code></pre>
<p>Dispatches cwise mul for “Dense*Dense&quot; and “Dense*Sparse“.</p>
<h3 id="__ror__">
<code><strong>ror</strong></code>
</h3>
<pre class="python"><code>__ror__(
    a,
    *args
)</code></pre>
<p>Returns the truth value of x OR y element-wise.</p>
<p><em>NOTE</em>: <code>LogicalOr</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-23">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-26">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__rpow__">
<code><strong>rpow</strong></code>
</h3>
<pre class="python"><code>__rpow__(
    a,
    *args
)</code></pre>
<p>Computes the power of one value to another.</p>
<p>Given a tensor <code>x</code> and a tensor <code>y</code>, this operation computes \(x^y\) for corresponding elements in <code>x</code> and <code>y</code>. For example:</p>
<pre class="python"><code>x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
tf.pow(x, y)  # [[256, 65536], [9, 27]]</code></pre>
<h4 id="args-24">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-27">Returns:</h4>
<p>A <code>Tensor</code>.</p>
<h3 id="__rsub__">
<code><strong>rsub</strong></code>
</h3>
<pre class="python"><code>__rsub__(
    a,
    *args
)</code></pre>
<p>Returns x - y element-wise.</p>
<p><em>NOTE</em>: <code>Subtract</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-25">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>bfloat16</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>uint16</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-28">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__rtruediv__">
<code><strong>rtruediv</strong></code>
</h3>
<pre class="python"><code>__rtruediv__(
    a,
    *args
)</code></pre>
<h3 id="__rxor__">
<code><strong>rxor</strong></code>
</h3>
<pre class="python"><code>__rxor__(
    a,
    *args
)</code></pre>
<p>x ^ y = (x | y) &amp; ~(x &amp; y).</p>
<h3 id="__sub__">
<code><strong>sub</strong></code>
</h3>
<pre class="python"><code>__sub__(
    a,
    *args
)</code></pre>
<p>Returns x - y element-wise.</p>
<p><em>NOTE</em>: <code>Subtract</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-26">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>bfloat16</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>uint16</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-29">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__truediv__">
<code><strong>truediv</strong></code>
</h3>
<pre class="python"><code>__truediv__(
    a,
    *args
)</code></pre>
<h3 id="__xor__">
<code><strong>xor</strong></code>
</h3>
<pre class="python"><code>__xor__(
    a,
    *args
)</code></pre>
<p>x ^ y = (x | y) &amp; ~(x &amp; y).</p>
<h3 id="batch_shape_tensor">
<code>batch_shape_tensor</code>
</h3>
<pre class="python"><code>batch_shape_tensor(name=&#39;batch_shape_tensor&#39;)</code></pre>
<p>Shape of a single sample from a single event index as a 1-D <code>Tensor</code>.</p>
<p>The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.</p>
<h4 id="args-27">Args:</h4>
<ul>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h4 id="returns-30">Returns:</h4>
<ul>
<li><b><code>batch_shape</code></b>: <code>Tensor</code>.</li>
</ul>
<h3 id="cdf">
<code>cdf</code>
</h3>
<pre class="python"><code>cdf(
    value,
    name=&#39;cdf&#39;
)</code></pre>
<p>Cumulative distribution function.</p>
<p>Given random variable <code>X</code>, the cumulative distribution function <code>cdf</code> is:</p>
<pre class="none"><code>cdf(x) := P[X &lt;= x]</code></pre>
<h4 id="args-28">Args:</h4>
<ul>
<li><b><code>value</code></b>: <code>float</code> or <code>double</code> <code>Tensor</code>.</li>
<li><b><code>name</code></b>: Python <code>str</code> prepended to names of ops created by this function.</li>
</ul>
<h4 id="returns-31">Returns:</h4>
<ul>
<li><b><code>cdf</code></b>: a <code>Tensor</code> of shape <code>sample_shape(x) + self.batch_shape</code> with values of type <code>self.dtype</code>.</li>
</ul>
<h3 id="conjugate_log_prob">
<code>conjugate_log_prob</code>
</h3>
<pre class="python"><code>conjugate_log_prob(val=None)</code></pre>
<h3 id="copy">
<code>copy</code>
</h3>
<pre class="python"><code>copy(**override_parameters_kwargs)</code></pre>
<p>Creates a deep copy of the distribution.</p>
<p>Note: the copy distribution may continue to depend on the original initialization arguments.</p>
<h4 id="args-29">Args:</h4>
<p>**override_parameters_kwargs: String/value dictionary of initialization arguments to override with new values.</p>
<h4 id="returns-32">Returns:</h4>
<ul>
<li><b><code>distribution</code></b>: A new instance of <code>type(self)</code> initialized from the union of self.parameters and override_parameters_kwargs, i.e., <code>dict(self.parameters, **override_parameters_kwargs)</code>.</li>
</ul>
<h3 id="covariance">
<code>covariance</code>
</h3>
<pre class="python"><code>covariance(name=&#39;covariance&#39;)</code></pre>
<p>Covariance.</p>
<p>Covariance is (possibly) defined only for non-scalar-event distributions.</p>
<p>For example, for a length-<code>k</code>, vector-valued distribution, it is calculated as,</p>
<pre class="none"><code>Cov[i, j] = Covariance(X_i, X_j) = E[(X_i - E[X_i]) (X_j - E[X_j])]</code></pre>
<p>where <code>Cov</code> is a (batch of) <code>k x k</code> matrix, <code>0 &lt;= (i, j) &lt; k</code>, and <code>E</code> denotes expectation.</p>
<p>Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), <code>Covariance</code> shall return a (batch of) matrices under some vectorization of the events, i.e.,</p>
<pre class="none"><code>Cov[i, j] = Covariance(Vec(X)_i, Vec(X)_j) = [as above]</code></pre>
<p>where <code>Cov</code> is a (batch of) <code>k' x k'</code> matrices, <code>0 &lt;= (i, j) &lt; k' = reduce_prod(event_shape)</code>, and <code>Vec</code> is some function mapping indices of this distribution’s event dimensions to indices of a length-<code>k'</code> vector.</p>
<h4 id="args-30">Args:</h4>
<ul>
<li><b><code>name</code></b>: Python <code>str</code> prepended to names of ops created by this function.</li>
</ul>
<h4 id="returns-33">Returns:</h4>
<ul>
<li><b><code>covariance</code></b>: Floating-point <code>Tensor</code> with shape <code>[B1, ..., Bn, k', k']</code> where the first <code>n</code> dimensions are batch coordinates and <code>k' = reduce_prod(self.event_shape)</code>.</li>
</ul>
<h3 id="cross_entropy">
<code>cross_entropy</code>
</h3>
<pre class="python"><code>cross_entropy(
    other,
    name=&#39;cross_entropy&#39;
)</code></pre>
<p>Computes the (Shannon) cross entropy.</p>
<p>Denote this distribution (<code>self</code>) by <code>P</code> and the <code>other</code> distribution by <code>Q</code>. Assuming <code>P, Q</code> are absolutely continuous with respect to one another and permit densities <code>p(x) dr(x)</code> and <code>q(x) dr(x)</code>, (Shanon) cross entropy is defined as:</p>
<pre class="none"><code>H[P, Q] = E_p[-log q(X)] = -int_F p(x) log q(x) dr(x)</code></pre>
<p>where <code>F</code> denotes the support of the random variable <code>X ~ P</code>.</p>
<h4 id="args-31">Args:</h4>
<ul>
<li><b><code>other</code></b>: <code>tf.distributions.Distribution</code> instance.</li>
<li><b><code>name</code></b>: Python <code>str</code> prepended to names of ops created by this function.</li>
</ul>
<h4 id="returns-34">Returns:</h4>
<ul>
<li><b><code>cross_entropy</code></b>: <code>self.dtype</code> <code>Tensor</code> with shape <code>[B1, ..., Bn]</code> representing <code>n</code> different calculations of (Shanon) cross entropy.</li>
</ul>
<h3 id="entropy">
<code>entropy</code>
</h3>
<pre class="python"><code>entropy(name=&#39;entropy&#39;)</code></pre>
<p>Shannon entropy in nats.</p>
<h3 id="eval">
<code>eval</code>
</h3>
<pre class="python"><code>eval(
    session=None,
    feed_dict=None
)</code></pre>
<p>In a session, computes and returns the value of this random variable.</p>
<p>This is not a graph construction method, it does not add ops to the graph.</p>
<p>This convenience method requires a session where the graph containing this variable has been launched. If no session is passed, the default session is used.</p>
<h4 id="args-32">Args:</h4>
<ul>
<li><b><code>session</code></b>: tf.BaseSession. The <code>tf.Session</code> to use to evaluate this random variable. If none, the default session is used.</li>
<li><b><code>feed_dict</code></b>: dict. A dictionary that maps <code>tf.Tensor</code> objects to feed values. See <code>tf.Session.run()</code> for a description of the valid feed values.</li>
</ul>
<h4 id="examples">Examples</h4>
<pre class="python"><code>x = Normal(0.0, 1.0)
with tf.Session() as sess:
  # Usage passing the session explicitly.
  print(x.eval(sess))
  # Usage with the default session.  The &#39;with&#39; block
  # above makes &#39;sess&#39; the default session.
  print(x.eval())</code></pre>
<h3 id="event_shape_tensor">
<code>event_shape_tensor</code>
</h3>
<pre class="python"><code>event_shape_tensor(name=&#39;event_shape_tensor&#39;)</code></pre>
<p>Shape of a single sample from a single batch as a 1-D int32 <code>Tensor</code>.</p>
<h4 id="args-33">Args:</h4>
<ul>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h4 id="returns-35">Returns:</h4>
<ul>
<li><b><code>event_shape</code></b>: <code>Tensor</code>.</li>
</ul>
<h3 id="get_ancestors">
<code>get_ancestors</code>
</h3>
<pre class="python"><code>get_ancestors(collection=None)</code></pre>
<p>Get ancestor random variables.</p>
<h3 id="get_blanket">
<code>get_blanket</code>
</h3>
<pre class="python"><code>get_blanket(collection=None)</code></pre>
<p>Get the random variable’s Markov blanket.</p>
<h3 id="get_children">
<code>get_children</code>
</h3>
<pre class="python"><code>get_children(collection=None)</code></pre>
<p>Get child random variables.</p>
<h3 id="get_descendants">
<code>get_descendants</code>
</h3>
<pre class="python"><code>get_descendants(collection=None)</code></pre>
<p>Get descendant random variables.</p>
<h3 id="get_parents">
<code>get_parents</code>
</h3>
<pre class="python"><code>get_parents(collection=None)</code></pre>
<p>Get parent random variables.</p>
<h3 id="get_shape">
<code>get_shape</code>
</h3>
<pre class="python"><code>get_shape()</code></pre>
<p>Get shape of random variable.</p>
<h3 id="get_siblings">
<code>get_siblings</code>
</h3>
<pre class="python"><code>get_siblings(collection=None)</code></pre>
<p>Get sibling random variables.</p>
<h3 id="get_variables">
<code>get_variables</code>
</h3>
<pre class="python"><code>get_variables(collection=None)</code></pre>
<p>Get TensorFlow variables that the random variable depends on.</p>
<h3 id="is_scalar_batch">
<code>is_scalar_batch</code>
</h3>
<pre class="python"><code>is_scalar_batch(name=&#39;is_scalar_batch&#39;)</code></pre>
<p>Indicates that <code>batch_shape == []</code>.</p>
<h4 id="args-34">Args:</h4>
<ul>
<li><b><code>name</code></b>: Python <code>str</code> prepended to names of ops created by this function.</li>
</ul>
<h4 id="returns-36">Returns:</h4>
<ul>
<li><b><code>is_scalar_batch</code></b>: <code>bool</code> scalar <code>Tensor</code>.</li>
</ul>
<h3 id="is_scalar_event">
<code>is_scalar_event</code>
</h3>
<pre class="python"><code>is_scalar_event(name=&#39;is_scalar_event&#39;)</code></pre>
<p>Indicates that <code>event_shape == []</code>.</p>
<h4 id="args-35">Args:</h4>
<ul>
<li><b><code>name</code></b>: Python <code>str</code> prepended to names of ops created by this function.</li>
</ul>
<h4 id="returns-37">Returns:</h4>
<ul>
<li><b><code>is_scalar_event</code></b>: <code>bool</code> scalar <code>Tensor</code>.</li>
</ul>
<h3 id="kl_divergence">
<code>kl_divergence</code>
</h3>
<pre class="python"><code>kl_divergence(
    other,
    name=&#39;kl_divergence&#39;
)</code></pre>
<p>Computes the Kullback–Leibler divergence.</p>
<p>Denote this distribution (<code>self</code>) by <code>p</code> and the <code>other</code> distribution by <code>q</code>. Assuming <code>p, q</code> are absolutely continuous with respect to reference measure <code>r</code>, (Shanon) cross entropy is defined as:</p>
<pre class="none"><code>KL[p, q] = E_p[log(p(X)/q(X))]
         = -int_F p(x) log q(x) dr(x) + int_F p(x) log p(x) dr(x)
         = H[p, q] - H[p]</code></pre>
<p>where <code>F</code> denotes the support of the random variable <code>X ~ p</code>, <code>H[., .]</code> denotes (Shanon) cross entropy, and <code>H[.]</code> denotes (Shanon) entropy.</p>
<h4 id="args-36">Args:</h4>
<ul>
<li><b><code>other</code></b>: <code>tf.distributions.Distribution</code> instance.</li>
<li><b><code>name</code></b>: Python <code>str</code> prepended to names of ops created by this function.</li>
</ul>
<h4 id="returns-38">Returns:</h4>
<ul>
<li><b><code>kl_divergence</code></b>: <code>self.dtype</code> <code>Tensor</code> with shape <code>[B1, ..., Bn]</code> representing <code>n</code> different calculations of the Kullback-Leibler divergence.</li>
</ul>
<h3 id="log_cdf">
<code>log_cdf</code>
</h3>
<pre class="python"><code>log_cdf(
    value,
    name=&#39;log_cdf&#39;
)</code></pre>
<p>Log cumulative distribution function.</p>
<p>Given random variable <code>X</code>, the cumulative distribution function <code>cdf</code> is:</p>
<pre class="none"><code>log_cdf(x) := Log[ P[X &lt;= x] ]</code></pre>
<p>Often, a numerical approximation can be used for <code>log_cdf(x)</code> that yields a more accurate answer than simply taking the logarithm of the <code>cdf</code> when <code>x &lt;&lt; -1</code>.</p>
<h4 id="args-37">Args:</h4>
<ul>
<li><b><code>value</code></b>: <code>float</code> or <code>double</code> <code>Tensor</code>.</li>
<li><b><code>name</code></b>: Python <code>str</code> prepended to names of ops created by this function.</li>
</ul>
<h4 id="returns-39">Returns:</h4>
<ul>
<li><b><code>logcdf</code></b>: a <code>Tensor</code> of shape <code>sample_shape(x) + self.batch_shape</code> with values of type <code>self.dtype</code>.</li>
</ul>
<h3 id="log_prob">
<code>log_prob</code>
</h3>
<pre class="python"><code>log_prob(
    value,
    name=&#39;log_prob&#39;
)</code></pre>
<p>Log probability density/mass function.</p>
<h4 id="args-38">Args:</h4>
<ul>
<li><b><code>value</code></b>: <code>float</code> or <code>double</code> <code>Tensor</code>.</li>
<li><b><code>name</code></b>: Python <code>str</code> prepended to names of ops created by this function.</li>
</ul>
<h4 id="returns-40">Returns:</h4>
<ul>
<li><b><code>log_prob</code></b>: a <code>Tensor</code> of shape <code>sample_shape(x) + self.batch_shape</code> with values of type <code>self.dtype</code>.</li>
</ul>
<h3 id="log_survival_function">
<code>log_survival_function</code>
</h3>
<pre class="python"><code>log_survival_function(
    value,
    name=&#39;log_survival_function&#39;
)</code></pre>
<p>Log survival function.</p>
<p>Given random variable <code>X</code>, the survival function is defined:</p>
<pre class="none"><code>log_survival_function(x) = Log[ P[X &gt; x] ]
                         = Log[ 1 - P[X &lt;= x] ]
                         = Log[ 1 - cdf(x) ]</code></pre>
<p>Typically, different numerical approximations can be used for the log survival function, which are more accurate than <code>1 - cdf(x)</code> when <code>x &gt;&gt; 1</code>.</p>
<h4 id="args-39">Args:</h4>
<ul>
<li><b><code>value</code></b>: <code>float</code> or <code>double</code> <code>Tensor</code>.</li>
<li><b><code>name</code></b>: Python <code>str</code> prepended to names of ops created by this function.</li>
</ul>
<h4 id="returns-41">Returns:</h4>
<p><code>Tensor</code> of shape <code>sample_shape(x) + self.batch_shape</code> with values of type <code>self.dtype</code>.</p>
<h3 id="mean">
<code>mean</code>
</h3>
<pre class="python"><code>mean(name=&#39;mean&#39;)</code></pre>
<p>Mean.</p>
<h3 id="mode">
<code>mode</code>
</h3>
<pre class="python"><code>mode(name=&#39;mode&#39;)</code></pre>
<p>Mode.</p>
<p>Additional documentation from <code>Gamma</code>:</p>
<p>The mode of a gamma distribution is <code>(shape - 1) / rate</code> when <code>shape &gt; 1</code>, and <code>NaN</code> otherwise. If <code>self.allow_nan_stats</code> is <code>False</code>, an exception will be raised rather than returning <code>NaN</code>.</p>
<h3 id="param_shapes">
<code>param_shapes</code>
</h3>
<pre class="python"><code>param_shapes(
    cls,
    sample_shape,
    name=&#39;DistributionParamShapes&#39;
)</code></pre>
<p>Shapes of parameters given the desired shape of a call to <code>sample()</code>.</p>
<p>This is a class method that describes what key/value arguments are required to instantiate the given <code>Distribution</code> so that a particular shape is returned for that instance’s call to <code>sample()</code>.</p>
<p>Subclasses should override class method <code>_param_shapes</code>.</p>
<h4 id="args-40">Args:</h4>
<ul>
<li><b><code>sample_shape</code></b>: <code>Tensor</code> or python list/tuple. Desired shape of a call to <code>sample()</code>.</li>
<li><b><code>name</code></b>: name to prepend ops with.</li>
</ul>
<h4 id="returns-42">Returns:</h4>
<p><code>dict</code> of parameter name to <code>Tensor</code> shapes.</p>
<h3 id="param_static_shapes">
<code>param_static_shapes</code>
</h3>
<pre class="python"><code>param_static_shapes(
    cls,
    sample_shape
)</code></pre>
<p>param_shapes with static (i.e. <code>TensorShape</code>) shapes.</p>
<p>This is a class method that describes what key/value arguments are required to instantiate the given <code>Distribution</code> so that a particular shape is returned for that instance’s call to <code>sample()</code>. Assumes that the sample’s shape is known statically.</p>
<p>Subclasses should override class method <code>_param_shapes</code> to return constant-valued tensors when constant values are fed.</p>
<h4 id="args-41">Args:</h4>
<ul>
<li><b><code>sample_shape</code></b>: <code>TensorShape</code> or python list/tuple. Desired shape of a call to <code>sample()</code>.</li>
</ul>
<h4 id="returns-43">Returns:</h4>
<p><code>dict</code> of parameter name to <code>TensorShape</code>.</p>
<h4 id="raises-5">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: if <code>sample_shape</code> is a <code>TensorShape</code> and is not fully defined.</li>
</ul>
<h3 id="prob">
<code>prob</code>
</h3>
<pre class="python"><code>prob(
    value,
    name=&#39;prob&#39;
)</code></pre>
<p>Probability density/mass function.</p>
<h4 id="args-42">Args:</h4>
<ul>
<li><b><code>value</code></b>: <code>float</code> or <code>double</code> <code>Tensor</code>.</li>
<li><b><code>name</code></b>: Python <code>str</code> prepended to names of ops created by this function.</li>
</ul>
<h4 id="returns-44">Returns:</h4>
<ul>
<li><b><code>prob</code></b>: a <code>Tensor</code> of shape <code>sample_shape(x) + self.batch_shape</code> with values of type <code>self.dtype</code>.</li>
</ul>
<h3 id="quantile">
<code>quantile</code>
</h3>
<pre class="python"><code>quantile(
    value,
    name=&#39;quantile&#39;
)</code></pre>
<p>Quantile function. Aka “inverse cdf” or “percent point function”.</p>
<p>Given random variable <code>X</code> and <code>p in [0, 1]</code>, the <code>quantile</code> is:</p>
<pre class="none"><code>quantile(p) := x such that P[X &lt;= x] == p</code></pre>
<h4 id="args-43">Args:</h4>
<ul>
<li><b><code>value</code></b>: <code>float</code> or <code>double</code> <code>Tensor</code>.</li>
<li><b><code>name</code></b>: Python <code>str</code> prepended to names of ops created by this function.</li>
</ul>
<h4 id="returns-45">Returns:</h4>
<ul>
<li><b><code>quantile</code></b>: a <code>Tensor</code> of shape <code>sample_shape(x) + self.batch_shape</code> with values of type <code>self.dtype</code>.</li>
</ul>
<h3 id="sample">
<code>sample</code>
</h3>
<pre class="python"><code>sample(
    sample_shape=(),
    seed=None,
    name=&#39;sample&#39;
)</code></pre>
<p>Generate samples of the specified shape.</p>
<p>Note that a call to <code>sample()</code> without arguments will generate a single sample.</p>
<h4 id="args-44">Args:</h4>
<ul>
<li><b><code>sample_shape</code></b>: 0D or 1D <code>int32</code> <code>Tensor</code>. Shape of the generated samples.</li>
<li><b><code>seed</code></b>: Python integer seed for RNG</li>
<li><b><code>name</code></b>: name to give to the op.</li>
</ul>
<h4 id="returns-46">Returns:</h4>
<ul>
<li><b><code>samples</code></b>: a <code>Tensor</code> with prepended dimensions <code>sample_shape</code>.</li>
</ul>
<h3 id="stddev">
<code>stddev</code>
</h3>
<pre class="python"><code>stddev(name=&#39;stddev&#39;)</code></pre>
<p>Standard deviation.</p>
<p>Standard deviation is defined as,</p>
<pre class="none"><code>stddev = E[(X - E[X])**2]**0.5</code></pre>
<p>where <code>X</code> is the random variable associated with this distribution, <code>E</code> denotes expectation, and <code>stddev.shape = batch_shape + event_shape</code>.</p>
<h4 id="args-45">Args:</h4>
<ul>
<li><b><code>name</code></b>: Python <code>str</code> prepended to names of ops created by this function.</li>
</ul>
<h4 id="returns-47">Returns:</h4>
<ul>
<li><b><code>stddev</code></b>: Floating-point <code>Tensor</code> with shape identical to <code>batch_shape + event_shape</code>, i.e., the same shape as <code>self.mean()</code>.</li>
</ul>
<h3 id="survival_function">
<code>survival_function</code>
</h3>
<pre class="python"><code>survival_function(
    value,
    name=&#39;survival_function&#39;
)</code></pre>
<p>Survival function.</p>
<p>Given random variable <code>X</code>, the survival function is defined:</p>
<pre class="none"><code>survival_function(x) = P[X &gt; x]
                     = 1 - P[X &lt;= x]
                     = 1 - cdf(x).</code></pre>
<h4 id="args-46">Args:</h4>
<ul>
<li><b><code>value</code></b>: <code>float</code> or <code>double</code> <code>Tensor</code>.</li>
<li><b><code>name</code></b>: Python <code>str</code> prepended to names of ops created by this function.</li>
</ul>
<h4 id="returns-48">Returns:</h4>
<p><code>Tensor</code> of shape <code>sample_shape(x) + self.batch_shape</code> with values of type <code>self.dtype</code>.</p>
<h3 id="value">
<code>value</code>
</h3>
<pre class="python"><code>value()</code></pre>
<p>Get tensor that the random variable corresponds to.</p>
<h3 id="variance">
<code>variance</code>
</h3>
<pre class="python"><code>variance(name=&#39;variance&#39;)</code></pre>
<p>Variance.</p>
<p>Variance is defined as,</p>
<pre class="none"><code>Var = E[(X - E[X])**2]</code></pre>
<p>where <code>X</code> is the random variable associated with this distribution, <code>E</code> denotes expectation, and <code>Var.shape = batch_shape + event_shape</code>.</p>
<h4 id="args-47">Args:</h4>
<ul>
<li><b><code>name</code></b>: Python <code>str</code> prepended to names of ops created by this function.</li>
</ul>
<h4 id="returns-49">Returns:</h4>
<ul>
<li><b><code>variance</code></b>: Floating-point <code>Tensor</code> with shape identical to <code>batch_shape + event_shape</code>, i.e., the same shape as <code>self.mean()</code>.</li>
</ul>
<h2 id="class-members">Class Members</h2>
<h3 id="__array_priority__">
<code><strong>array_priority</strong></code>
</h3>
<h3 id="support">
<code>support</code>
</h3>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
