<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward – Inference</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="/css/normalize.css" rel="stylesheet">
<link href="/css/skeleton.css" rel="stylesheet">
<!-- Dynamically resize logo for mobile -->
<style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  /* Downgrade API's header styles without explicitly modifying their type. */
  div.nine.columns h1 { text-align: left; }
  div.nine.columns h1 { font-size: 3.0rem; line-height: 1.25; letter-spacing: -.1rem; }
  div.nine.columns h2 { font-size: 2.4rem; line-height: 1.3;  letter-spacing: -.1rem; }
  div.nine.columns h3 { font-size: 2.4rem; line-height: 1.35; letter-spacing: -.08rem; }
  div.nine.columns h4 { font-size: 1.8rem; line-height: 1.5;  letter-spacing: -.05rem; }
  div.nine.columns h5 { font-size: 1.5rem; line-height: 1.6;  letter-spacing: 0; }
  @media (min-width: 550px) {
    div.nine.columns h1 { font-size: 3.6rem; }
    div.nine.columns h2 { font-size: 3.0rem; }
    div.nine.columns h3 { font-size: 3.0rem; }
    div.nine.columns h4 { font-size: 2.5rem; }
    div.nine.columns h5 { font-size: 1.5rem; }
  }
  </style>
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="/icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="/icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="/icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="/icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="/icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="/icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="/icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="/icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="/icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="/icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="/icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="/icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="/icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="/icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="/icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="/icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="/">Edward</a></h1>
<a href="/">
<center>
<img alt="Edward" class="logo-width" src="/images/edward.png"/>
</center>
</a>
<a class="button u-full-width" href="/api/">API</a>
<hr style="margin-top: 1rem; margin-bottom: 1.5rem;"/>
<a class="button u-full-width" href="/api/reference">Reference</a>
<a class="button u-full-width" href="/api/ed/criticisms">ed.criticisms</a>
<a class="button u-full-width" href="/api/ed/inferences">ed.inferences</a>
<a class="button u-full-width" href="/api/ed/models">ed.models</a>
<a class="button u-full-width" href="/api/ed/util">ed.util</a>
<a class="button u-full-width" href="/api/observations">observations</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a class="button2 u-pull-right" href="https://github.com/blei-lab/edward" style="padding-right:10%">
<span style="vertical-align:middle;">Github</span> 
      <img alt="Edward on Github" src="/images/github-mark.svg" style="vertical-align:middle;"/>
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<h1 id="api-and-documentation">API and Documentation</h1>
<div class="row" style="padding-bottom: 5%">
<div class="row" style="padding-bottom: 1%">
<a class="button3" href="/api/data">Data</a>
<a class="button3" href="/api/model">Model</a>
<a class="button3 button-primary" href="/api/inference">Inference</a>
<a class="button3" href="/api/criticism">Criticism</a>
</div>
<div class="row">
<a class="button4" href="/api/inference-classes">Classes</a>
<a class="button4" href="/api/inference-compositionality">Compositionality</a>
<a class="button4" href="/api/inference-data-subsampling">Data Subsampling</a>
<a class="button4" href="/api/inference-development">Development</a>
</div>
</div>
<h3 id="inference">Inference</h3>
<p>We describe how to perform inference in probabilistic models. For background, see the <a href="/tutorials/inference">Inference tutorial</a>.</p>
<p>Suppose we have a model <span class="math inline">\(p(\mathbf{x}, \mathbf{z}, \beta)\)</span> of data <span class="math inline">\(\mathbf{x}_{\text{train}}\)</span> with latent variables <span class="math inline">\((\mathbf{z}, \beta)\)</span>. Consider the posterior inference problem, <span class="math display">\[q(\mathbf{z}, \beta)\approx p(\mathbf{z}, \beta\mid \mathbf{x}_{\text{train}}),\]</span> in which the task is to approximate the posterior <span class="math inline">\(p(\mathbf{z}, \beta\mid \mathbf{x}_{\text{train}})\)</span> using a family of distributions, <span class="math inline">\(q(\mathbf{z},\beta; \lambda)\)</span>, indexed by parameters <span class="math inline">\(\lambda\)</span>.</p>
<p>In Edward, let <code>z</code> and <code>beta</code> be latent variables in the model, where we observe the random variable <code>x</code> with data <code>x_train</code>. Let <code>qz</code> and <code>qbeta</code> be random variables defined to approximate the posterior. We write this problem as follows:</p>
<pre class="python" data-language="Python"><code>inference = ed.Inference({z: qz, beta: qbeta}, {x: x_train})</code></pre>
<p><code>Inference</code> is an abstract class which takes two inputs. The first is a collection of latent random variables <code>beta</code> and <code>z</code>, along with “posterior variables” <code>qbeta</code> and <code>qz</code>, which are associated to their respective latent variables. The second is a collection of observed random variables <code>x</code>, which is associated to the data <code>x_train</code>.</p>
<p>Inference adjusts parameters of the distribution of <code>qbeta</code> and <code>qz</code> to be close to the posterior <span class="math inline">\(p(\mathbf{z}, \beta\,|\,\mathbf{x}_{\text{train}})\)</span>.</p>
<p>Running inference is as simple as running one method.</p>
<pre class="python" data-language="Python"><code>inference = ed.Inference({z: qz, beta: qbeta}, {x: x_train})
inference.run()</code></pre>
<p>Inference also supports fine control of the training procedure.</p>
<pre class="python" data-language="Python"><code>inference = ed.Inference({z: qz, beta: qbeta}, {x: x_train})
inference.initialize()

tf.global_variables_initializer().run()

for _ in range(inference.n_iter):
  info_dict = inference.update()
  inference.print_progress(info_dict)

inference.finalize()</code></pre>
<p><code>initialize()</code> builds the algorithm’s update rules (computational graph) for <span class="math inline">\(\lambda\)</span>; <code>tf.global_variables_initializer().run()</code> initializes <span class="math inline">\(\lambda\)</span> (TensorFlow variables in the graph); <code>update()</code> runs the graph once to update <span class="math inline">\(\lambda\)</span>, which is called in a loop until convergence; <code>finalize()</code> runs any computation as the algorithm terminates.</p>
<p>The <code>run()</code> method is a simple wrapper for this procedure.</p>
<h3 id="other-settings">Other Settings</h3>
<p>We highlight other settings during inference.</p>
<p><strong>Model parameters</strong>. Model parameters are parameters in a model that we will always compute point estimates for and not be uncertain about. They are defined with <code>tf.Variable</code>s, where the inference problem is <span class="math display">\[\hat{\theta} \leftarrow^{\text{optimize}}
p(\mathbf{x}_{\text{train}}; \theta)\]</span></p>
<pre class="python" data-language="Python"><code>from edward.models import Normal

theta = tf.Variable(0.0)
x = Normal(loc=tf.ones(10) * theta, scale=1.0)

inference = ed.Inference({}, {x: x_train})</code></pre>
<p>Only a subset of inference algorithms support estimation of model parameters. (Note also that this inference example does not have any latent variables. It is only about estimating <code>theta</code> given that we observe <span class="math inline">\(\mathbf{x} = \mathbf{x}_{\text{train}}\)</span>. We can add them so that inference is both posterior inference and parameter estimation.)</p>
<p>For example, model parameters are useful when applying neural networks from high-level libraries such as Keras and TensorFlow Slim. See the <a href="/api/model-compositionality">model compositionality</a> page for more details.</p>
<p><strong>Conditional inference</strong>. In conditional inference, only a subset of the posterior is inferred while the rest are fixed using other inferences. The inference problem is <span class="math display">\[q(\mathbf{z}\mid\beta)q(\beta)\approx
p(\mathbf{z}, \beta\mid\mathbf{x}_{\text{train}})\]</span> where parameters in <span class="math inline">\(q(\mathbf{z}\mid\beta)\)</span> are estimated and <span class="math inline">\(q(\beta)\)</span> is fixed. In Edward, we enable conditioning by binding random variables to other random variables in <code>data</code>.</p>
<pre class="python" data-language="Python"><code>inference = ed.Inference({z: qz}, {x: x_train, beta: qbeta})</code></pre>
<p>In the <a href="/api/inference-compositionality">compositionality page</a>, we describe how to construct inference by composing many conditional inference algorithms.</p>
<p><strong>Implicit prior samples</strong>. Latent variables can be defined in the model without any posterior inference over them. They are implicitly marginalized out with a single sample. The inference problem is <span class="math display">\[q(\beta)\approx
p(\beta\mid\mathbf{x}_{\text{train}}, \mathbf{z}^*)\]</span> where <span class="math inline">\(\mathbf{z}^*\sim p(\mathbf{z}\mid\beta)\)</span> is a prior sample.</p>
<pre class="python" data-language="Python"><code>inference = ed.Inference({beta: qbeta}, {x: x_train})</code></pre>
<p>For example, implicit prior samples are useful for generative adversarial networks. Their inference problem does not require any inference over the latent variables; it uses samples from the prior.</p>
<hr/>
<ul>
<li><a href="./ed/Inference"><code>ed.inferences.Inference</code></a></li>
<li><a href="./ed/VariationalInference"><code>ed.inferences.VariationalInference</code></a>
<ul>
<li><a href="./ed/KLqp"><code>ed.inferences.KLqp</code></a>
<ul>
<li><a href="./ed/ReparameterizationKLqp"><code>ed.inferences.ReparameterizationKLqp</code></a></li>
<li><a href="./ed/ReparameterizationKLKLqp"><code>ed.inferences.ReparameterizationKLKLqp</code></a></li>
<li><a href="./ed/ReparameterizationEntropyKLqp"><code>ed.inferences.ReparameterizationEntropyKLqp</code></a></li>
<li><a href="./ed/ScoreKLqp"><code>ed.inferences.ScoreKLqp</code></a></li>
<li><a href="./ed/ScoreKLKLqp"><code>ed.inferences.ScoreKLKLqp</code></a></li>
<li><a href="./ed/ScoreEntropyKLqp"><code>ed.inferences.ScoreEntropyKLqp</code></a></li>
</ul></li>
<li><a href="./ed/KLpq"><code>ed.inferences.KLpq</code></a></li>
<li><a href="./ed/GANInference"><code>ed.inferences.GANInference</code></a>
<ul>
<li><a href="./ed/BiGANInference"><code>ed.inferences.BiGANInference</code></a></li>
<li><a href="./ed/ImplicitKLqp"><code>ed.inferences.ImplicitKLqp</code></a></li>
<li><a href="./ed/WGANInference"><code>ed.inferences.WGANInference</code></a></li>
</ul></li>
<li><a href="./ed/MAP"><code>ed.inferences.MAP</code></a>
<ul>
<li><a href="./ed/Laplace"><code>ed.inferences.Laplace</code></a></li>
</ul></li>
</ul></li>
<li><a href="./ed/MonteCarlo"><code>ed.inferences.MonteCarlo</code></a>
<ul>
<li><a href="./ed/Gibbs"><code>ed.inferences.Gibbs</code></a></li>
<li><a href="./ed/MetropolisHastings"><code>ed.inferences.MetropolisHastings</code></a></li>
<li><a href="./ed/HMC"><code>ed.inferences.HMC</code></a></li>
<li><a href="./ed/SGLD"><code>ed.inferences.SGLD</code></a></li>
<li><a href="./ed/SGHMC"><code>ed.inferences.SGHMC</code></a></li>
</ul></li>
<li><a href="./ed/complete_conditional"><code>ed.inferences.complete_conditional</code></a></li>
</ul>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
