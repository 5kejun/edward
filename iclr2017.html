<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward â€“ Deep Probabilistic Programming</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/getting-started">Getting Started</a>
    <a class="button u-full-width" href="/tutorials/">Tutorials</a>
    <a class="button u-full-width" href="/api/">API</a>
    <a class="button u-full-width" href="/community">Community</a>
    <a class="button u-full-width" href="/contributing">Contributing</a>
    <div class="row" style="padding-bottom: 5%"> </div>
    <a class="button2 u-pull-right" style="padding-right:10%"
      href="https://github.com/blei-lab/edward">
      <span style="vertical-align:middle;">Github</span>&nbsp;
      <img src="/images/github-mark.svg" style="vertical-align:middle;"
      alt="Edward on Github" />
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">

<h2 id="deep-probabilistic-programming">Deep Probabilistic Programming</h2>
<p>This webpage is a companion to the article, <a href="https://arxiv.org/abs/1701.03757">Deep Probabilistic Programming</a> <span class="citation" data-cites="tran2017deep">(Tran et al., 2017)</span>. Here we provide more details for plug-and-play with the code snippets. An interactive version with Jupyter notebook is available <a href="http://nbviewer.jupyter.org/github/blei-lab/edward/blob/master/notebooks/iclr2017.ipynb">here</a>.</p>
<p>The code snippets assume the following versions.</p>
<pre class="bash" data-language="bash"><code>pip install edward==1.3.1
pip install tensorflow==1.1.0  # alternatively, tensorflow-gpu==1.1.0
pip install keras==2.0.0</code></pre>
<h3 id="section-3.-compositional-representations-for-probabilistic-models">Section 3. Compositional Representations for Probabilistic Models</h3>
<p><strong>Figure 1</strong>. Beta-Bernoulli program.</p>
<pre data-language="python"><code>import tensorflow as tf
from edward.models import Bernoulli, Beta

theta = Beta(1.0, 1.0)
x = Bernoulli(tf.ones(50) * theta)</code></pre>
<p>For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/beta_bernoulli.py"><code>examples/beta_bernoulli.py</code></a> in the Github repository.</p>
<p><strong>Figure 2</strong>. Variational auto-encoder for a data set of 28 x 28 pixel images <span class="citation" data-cites="kingma2014auto rezende2014stochastic">(Kingma &amp; Welling, 2014; Rezende, Mohamed, &amp; Wierstra, 2014)</span>.</p>
<pre data-language="python"><code>import tensorflow as tf
from edward.models import Bernoulli, Normal
from keras.layers import Dense

N = 55000  # number of data points
d = 50  # latent dimension

# Probabilistic model
z = Normal(loc=tf.zeros([N, d]), scale=tf.ones([N, d]))
h = Dense(256, activation=&#39;relu&#39;)(z)
x = Bernoulli(logits=Dense(28 * 28, activation=None)(h))

# Variational model
qx = tf.placeholder(tf.float32, [N, 28 * 28])
qh = Dense(256, activation=&#39;relu&#39;)(qx)
qz = Normal(loc=Dense(d, activation=None)(qh),
            scale=Dense(d, activation=&#39;softplus&#39;)(qh))</code></pre>
<p>For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/vae.py"><code>examples/vae.py</code></a> in the Github repository.</p>
<p><strong>Figure 3</strong>. Bayesian recurrent neural network <span class="citation" data-cites="neal2012bayesian">(Neal, 2012)</span>. The program has an unspecified number of time steps; it uses a symbolic for loop (<code>tf.scan</code>).</p>
<pre data-language="python"><code>import edward as ed
import tensorflow as tf
from edward.models import Normal

H = 50  # number of hidden units
D = 10  # number of features

def rnn_cell(hprev, xt):
  return tf.tanh(ed.dot(hprev, Wh) + ed.dot(xt, Wx) + bh)

Wh = Normal(loc=tf.zeros([H, H]), scale=tf.ones([H, H]))
Wx = Normal(loc=tf.zeros([D, H]), scale=tf.ones([D, H]))
Wy = Normal(loc=tf.zeros([H, 1]), scale=tf.ones([H, 1]))
bh = Normal(loc=tf.zeros(H), scale=tf.ones(H))
by = Normal(loc=tf.zeros(1), scale=tf.ones(1))

x = tf.placeholder(tf.float32, [None, D])
h = tf.scan(rnn_cell, x, initializer=tf.zeros(H))
y = Normal(loc=tf.matmul(h, Wy) + by, scale=1.0)</code></pre>
<h3 id="section-4.-compositional-representations-for-inference">Section 4. Compositional Representations for Inference</h3>
<p><strong>Figure 5</strong>. Hierarchical model <span class="citation" data-cites="gelman2006data">(Gelman &amp; Hill, 2006)</span>. It is a mixture of Gaussians over <span class="math inline">\(D\)</span>-dimensional data <span class="math inline">\(\{x_n\}\in\mathbb{R}^{N\times D}\)</span>. There are <span class="math inline">\(K\)</span> latent cluster means <span class="math inline">\(\beta\in\mathbb{R}^{K\times D}\)</span>.</p>
<pre data-language="python"><code>import tensorflow as tf
from edward.models import Categorical, Normal

N = 10000  # number of data points
D = 2  # data dimension
K = 5  # number of clusters

beta = Normal(loc=tf.zeros([K, D]), scale=tf.ones([K, D]))
z = Categorical(logits=tf.zeros([N, K]))
x = Normal(loc=tf.gather(beta, z), scale=tf.ones([N, D]))</code></pre>
<p>It is used below in Figure 6 (left/right) and Figure * (variational EM).</p>
<p><strong>Figure 6</strong> <strong>(left)</strong>. Variational inference <span class="citation" data-cites="jordan1999introduction">(Jordan, Ghahramani, Jaakkola, &amp; Saul, 1999)</span>. It performs inference on the model defined in Figure 5.</p>
<pre data-language="python"><code>import edward as ed
import numpy as np
import tensorflow as tf
from edward.models import Categorical, Normal

x_train = np.zeros([N, D])

qbeta = Normal(loc=tf.Variable(tf.zeros([K, D])),
               scale=tf.exp(tf.Variable(tf.zeros([K, D]))))
qz = Categorical(logits=tf.Variable(tf.zeros([N, K])))

inference = ed.VariationalInference({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p><strong>Figure 6</strong> <strong>(right)</strong>. Monte Carlo <span class="citation" data-cites="robert1999monte">(Robert &amp; Casella, 1999)</span>. It performs inference on the model defined in Figure 5.</p>
<pre data-language="python"><code>import edward as ed
import numpy as np
import tensorflow as tf
from edward.models import Empirical

x_train = np.zeros([N, D])

T = 10000  # number of samples
qbeta = Empirical(params=tf.Variable(tf.zeros([T, K, D])))
qz = Empirical(params=tf.Variable(tf.zeros([T, N])))

inference = ed.MonteCarlo({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p><strong>Figure 7</strong>. Generative adversarial network <span class="citation" data-cites="goodfellow2014generative">(Goodfellow et al., 2014)</span>.</p>
<pre data-language="python"><code>import edward as ed
import numpy as np
import tensorflow as tf
from edward.models import Normal
from keras.layers import Dense

N = 55000  # number of data points
d = 50  # latent dimension

def generative_network(eps):
  h = Dense(256, activation=&#39;relu&#39;)(eps)
  return Dense(28 * 28, activation=None)(h)

def discriminative_network(x):
  h = Dense(28 * 28, activation=&#39;relu&#39;)(x)
  return Dense(1, activation=None)(h)

# DATA
x_train = np.zeros([N, 28 * 28])

# MODEL
eps = Normal(loc=tf.zeros([N, d]), scale=tf.ones([N, d]))
x = generative_network(eps)

# INFERENCE
inference = ed.GANInference(data={x: x_train},
                            discriminator=discriminative_network)</code></pre>
<p>For an example of it in use, see the <a href="/tutorials/gan">generative adversarial networks</a> tutorial.</p>
<p><strong>Figure *</strong>. Variational EM <span class="citation" data-cites="neal1993new">(Neal &amp; Hinton, 1993)</span>. It performs inference on the model defined in Figure 5.</p>
<pre data-language="python"><code>import edward as ed
import numpy as np
import tensorflow as tf
from edward.models import Categorical, PointMass

# DATA
x_train = np.zeros([N, D])

# INFERENCE
qbeta = PointMass(params=tf.Variable(tf.zeros([K, D])))
qz = Categorical(logits=tf.Variable(tf.zeros([N, K])))

inference_e = ed.VariationalInference({z: qz}, data={x: x_train, beta: qbeta})
inference_m = ed.MAP({beta: qbeta}, data={x: x_train, z: qz})

inference_e.initialize()
inference_m.initialize()

tf.initialize_all_variables().run()

for _ in range(10000):
  inference_e.update()
  inference_m.update()</code></pre>
<p>For more details, see the <a href="/api/inference-compositionality">inference compositionality</a> webpage. See <a href="https://github.com/blei-lab/edward/blob/master/examples/factor_analysis.py"><code>examples/factor_analysis.py</code></a> for a version performing Monte Carlo EM for logistic factor analysis in the Github repository. It leverages Hamiltonian Monte Carlo for the E-step to perform maximum marginal a posteriori.</p>
<p><strong>Figure *</strong>. Data subsampling.</p>
<pre data-language="python"><code>import edward as ed
import tensorflow as tf
from edward.models import Categorical, Normal

N = 10000  # number of data points
M = 128  # batch size during training
D = 2  # data dimension
K = 5  # number of clusters

# DATA
x_batch = tf.placeholder(tf.float32, [M, D])

# MODEL
beta = Normal(loc=tf.zeros([K, D]), scale=tf.ones([K, D]))
z = Categorical(logits=tf.zeros([M, K]))
x = Normal(loc=tf.gather(beta, z), scale=tf.ones([M, D]))

# INFERENCE
qbeta = Normal(loc=tf.Variable(tf.zeros([K, D])),
               scale=tf.nn.softplus(tf.Variable(tf.zeros([K, D]))))
qz = Categorical(logits=tf.Variable(tf.zeros([M, D])))

inference = ed.VariationalInference({beta: qbeta, z: qz}, data={x: x_batch})
inference.initialize(scale={x: float(N) / M, z: float(N) / M})</code></pre>
<p>For more details, see the <a href="/api/inference-data-subsampling">data subsampling</a> webpage.</p>
<h3 id="section-5.-experiments">Section 5. Experiments</h3>
<p><strong>Figure 9</strong>. Bayesian logistic regression with Hamiltonian Monte Carlo.</p>
<pre data-language="python"><code>import edward as ed
import numpy as np
import tensorflow as tf
from edward.models import Bernoulli, Empirical, Normal

N = 581012  # number of data points
D = 54  # number of features
T = 100  # number of empirical samples

# DATA
x_data = np.zeros([N, D])
y_data = np.zeros([N])

# MODEL
x = tf.Variable(x_data, trainable=False)
beta = Normal(loc=tf.zeros(D), scale=tf.ones(D))
y = Bernoulli(logits=ed.dot(x, beta))

# INFERENCE
qbeta = Empirical(params=tf.Variable(tf.zeros([T, D])))
inference = ed.HMC({beta: qbeta}, data={y: y_data})
inference.run(step_size=0.5 / N, n_steps=10)</code></pre>
<p>For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/bayesian_logistic_regression.py"><code>examples/bayesian_logistic_regression.py</code></a> in the Github repository.</p>
<h3 id="appendix-a.-model-examples">Appendix A. Model Examples</h3>
<p><strong>Figure 10</strong>. Bayesian neural network for classification <span class="citation" data-cites="denker1987large">(Denker, Schwartz, Wittner, &amp; Solla, 1987)</span>.</p>
<pre data-language="python"><code>import tensorflow as tf
from edward.models import Bernoulli, Normal

N = 1000  # number of data points
D = 528  # number of features
H = 256  # hidden layer size

W_0 = Normal(loc=tf.zeros([D, H]), scale=tf.ones([D, H]))
W_1 = Normal(loc=tf.zeros([H, 1]), scale=tf.ones([H, 1]))
b_0 = Normal(loc=tf.zeros(H), scale=tf.ones(H))
b_1 = Normal(loc=tf.zeros(1), scale=tf.ones(1))

x = tf.placeholder(tf.float32, [N, D])
y = Bernoulli(logits=tf.matmul(tf.nn.tanh(tf.matmul(x, W_0) + b_0), W_1) + b_1)</code></pre>
<p>For an example of it in use, see <a href="https://github.com/blei-lab/edward/blob/master/examples/getting_started_example.py"><code>examples/getting_started_example.py</code></a> in the Github repository.</p>
<p><strong>Figure 11</strong>. Latent Dirichlet allocation <span class="citation" data-cites="blei2003latent">(Blei, Ng, &amp; Jordan, 2003)</span>.</p>
<pre data-language="python"><code>import tensorflow as tf
from edward.models import Categorical, Dirichlet

D = 4  # number of documents
N = [11502, 213, 1523, 1351]  # words per doc
K = 10  # number of topics
V = 100000  # vocabulary size

theta = Dirichlet(tf.zeros([D, K]) + 0.1)
phi = Dirichlet(tf.zeros([K, V]) + 0.05)
z = [[0] * N] * D
w = [[0] * N] * D
for d in range(D):
  for n in range(N[d]):
    z[d][n] = Categorical(theta[d, :])
    w[d][n] = Categorical(phi[z[d][n], :])</code></pre>
<p><strong>Figure 12</strong>. Gaussian matrix factorization <span class="citation" data-cites="salakhutdinov2011probabilistic">(Salakhutdinov &amp; Mnih, 2011)</span>.</p>
<pre data-language="python"><code>import tensorflow as tf
from edward.models import Normal

N = 10
M = 10
K = 5  # latent dimension

U = Normal(loc=tf.zeros([M, K]), scale=tf.ones([M, K]))
V = Normal(loc=tf.zeros([N, K]), scale=tf.ones([N, K]))
Y = Normal(loc=tf.matmul(U, V, transpose_b=True), scale=tf.ones([N, M]))</code></pre>
<p><strong>Figure 13</strong>. Dirichlet process mixture model <span class="citation" data-cites="antoniak1974mixtures">(Antoniak, 1974)</span>.</p>
<pre data-language="python"><code>import tensorflow as tf
from edward.models import DirichletProcess, Normal

N = 1000  # number of data points
D = 5  # data dimensionality

dp = DirichletProcess(alpha=1.0, base=Normal(loc=tf.zeros(D), scale=tf.ones(D)))
mu = dp.sample(N)
x = Normal(loc=mu, scale=tf.ones([N, D]))</code></pre>
<p>To see the essential component defining the <code>DirichletProcess</code>, see <a href="https://github.com/blei-lab/edward/blob/master/examples/pp_dirichlet_process.py"><code>examples/pp_dirichlet_process.py</code></a> in the Github repository. Its source implementation can be found at <a href="https://github.com/blei-lab/edward/blob/master/edward/models/dirichlet_process.py"><code>edward/models/dirichlet_process.py</code></a>.</p>
<h3 id="appendix-b.-inference-examples">Appendix B. Inference Examples</h3>
<p><strong>Figure *</strong>. Stochastic variational inference <span class="citation" data-cites="hoffman2013stochastic">(Hoffman, Blei, Wang, &amp; Paisley, 2013)</span>. For more details, see the <a href="/api/inference-data-subsampling">data subsampling</a> webpage.</p>
<h3 id="appendix-c.-complete-examples">Appendix C. Complete Examples</h3>
<p><strong>Figure 15</strong>. Variational auto-encoder <span class="citation" data-cites="kingma2014auto rezende2014stochastic">(Kingma &amp; Welling, 2014; Rezende et al., 2014)</span>. See the script <a href="https://github.com/blei-lab/edward/blob/master/examples/vae.py"><code>examples/vae.py</code></a> in the Github repository.</p>
<p><strong>Figure 16</strong>. Exponential family embedding <span class="citation" data-cites="rudolph2016exponential">(Rudolph, Ruiz, Mandt, &amp; Blei, 2016)</span>. A Github repository with comprehensive features is available at <a href="https://github.com/mariru/exponential_family_embeddings">mariru/exponential_family_embeddings</a>.</p>
<h3 id="references" class="unnumbered">References</h3>
<div id="refs" class="references">
<div id="ref-antoniak1974mixtures">
<p>Antoniak, C. E. (1974). Mixtures of dirichlet processes with applications to bayesian nonparametric problems. <em>The Annals of Statistics</em>, 1152â€“1174.</p>
</div>
<div id="ref-blei2003latent">
<p>Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent Dirichlet Allocation. <em>Journal of Machine Learning Research</em>, <em>3</em>, 993â€“1022.</p>
</div>
<div id="ref-denker1987large">
<p>Denker, J., Schwartz, D., Wittner, B., &amp; Solla, S. (1987). Large automatic learning, rule extraction, and generalization. <em>Complex Systems</em>.</p>
</div>
<div id="ref-gelman2006data">
<p>Gelman, A., &amp; Hill, J. L. (2006). <em>Data analysis using regression and multilevel/hierarchical models</em>. Cambridge University Press.</p>
</div>
<div id="ref-goodfellow2014generative">
<p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., â€¦ Bengio, Y. (2014). Generative adversarial nets. In <em>Neural information processing systems</em>.</p>
</div>
<div id="ref-hoffman2013stochastic">
<p>Hoffman, M. D., Blei, D. M., Wang, C., &amp; Paisley, J. (2013). Stochastic variational inference. <em>The Journal of Machine Learning Research</em>, <em>14</em>(1), 1303â€“1347.</p>
</div>
<div id="ref-jordan1999introduction">
<p>Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., &amp; Saul, L. K. (1999). An introduction to variational methods for graphical models. <em>Machine Learning</em>, <em>37</em>(2), 183â€“233.</p>
</div>
<div id="ref-kingma2014auto">
<p>Kingma, D., &amp; Welling, M. (2014). Auto-encoding variational Bayes. In <em>International conference on learning representations</em>.</p>
</div>
<div id="ref-neal2012bayesian">
<p>Neal, R. M. (2012). <em>Bayesian learning for neural networks</em> (Vol. 118). Springer Science &amp; Business Media.</p>
</div>
<div id="ref-neal1993new">
<p>Neal, R. M., &amp; Hinton, G. E. (1993). A new view of the EM algorithm that justifies incremental and other variants. In <em>Learning in graphical models</em> (pp. 355â€“368).</p>
</div>
<div id="ref-rezende2014stochastic">
<p>Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In <em>ICML</em> (pp. 1278â€“1286).</p>
</div>
<div id="ref-robert1999monte">
<p>Robert, C. P., &amp; Casella, G. (1999). <em>Monte carlo statistical methods</em>. Springer.</p>
</div>
<div id="ref-rudolph2016exponential">
<p>Rudolph, M. R., Ruiz, F. J. R., Mandt, S., &amp; Blei, D. M. (2016). Exponential Family Embeddings. In <em>Neural information processing systems</em>.</p>
</div>
<div id="ref-salakhutdinov2011probabilistic">
<p>Salakhutdinov, R., &amp; Mnih, A. (2011). Probabilistic matrix factorization. In <em>Neural information processing systems</em>.</p>
</div>
<div id="ref-tran2017deep">
<p>Tran, D., Hoffman, M. D., Saurous, R. A., Brevdo, E., Murphy, K., &amp; Blei, D. M. (2017). Deep probabilistic programming. In <em>International conference on learning representations</em>.</p>
</div>
</div>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
