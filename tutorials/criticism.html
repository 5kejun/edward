<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward – Model Criticism</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/getting-started">Getting Started</a>
    <a class="button u-full-width" href="/tutorials/">Tutorials</a>
    <a class="button u-full-width" href="/api/">API</a>
    <a class="button u-full-width" href="/community">Community</a>
    <a class="button u-full-width" href="/contributing">Contributing</a>
    <div class="row" style="padding-bottom: 5%"> </div>
    <a class="button2 u-pull-right" style="padding-right:10%"
      href="https://github.com/blei-lab/edward">
      <span style="vertical-align:middle;">Github</span>&nbsp;
      <img src="/images/github-mark.svg" style="vertical-align:middle;"
      alt="Edward on Github" />
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">

<h2 id="model-criticism">Model Criticism</h2>
<p>We can never validate whether a model is true. In practice, “all models are wrong” <span class="citation" data-cites="box1976science">(Box, 1976)</span>. However, we can try to uncover where the model goes wrong. Model criticism helps justify the model as an approximation or point to good directions for revising the model.</p>
<p>Model criticism typically analyzes the posterior predictive distribution, <span class="math display">\[\begin{aligned}
  p(\mathbf{x}_\text{new} \mid \mathbf{x})
  &amp;=
  \int
  p(\mathbf{x}_\text{new} \mid \mathbf{z})
  p(\mathbf{z} \mid \mathbf{x})
  \text{d} \mathbf{z}.\end{aligned}\]</span> The model’s posterior predictive can be used to generate new data given past observations and can also make predictions on new data given past observations. It is formed by calculating the likelihood of the new data, averaged over every set of latent variables according to the posterior distribution.</p>
<p>A helpful utility function to form the posterior predictive is <code>copy</code>. For example, assume the model defines a likelihood <code>x</code> connected to a prior <code>z</code>. The posterior predictive distribution is</p>
<pre class="python" data-language="Python"><code>x_post = ed.copy(x, {z: qz})</code></pre>
<p>Here, we copy the likelihood node <code>x</code> in the graph and replace dependence on the prior <code>z</code> with dependence on the inferred posterior <code>qz</code>. We describe several techniques for model criticism.</p>
<h3 id="point-evaluation">Point Evaluation</h3>
<p>A point evaluation is a scalar-valued metric for assessing trained models <span class="citation" data-cites="winkler1994evaluating gneiting2007strictly">(Gneiting &amp; Raftery, 2007; Winkler, 1994)</span>. For example, we can assess models for classification by predicting the label for each observation in the data and comparing it to their true labels. Edward implements a variety of metrics, such as classification error and mean absolute error.</p>
<p>The <code>ed.evaluate()</code> method takes as input a set of metrics to evaluate, and a data dictionary. As with inference, the data dictionary binds the observed random variables in the model to realizations: in this case, it is the posterior predictive random variable of outputs <code>y_post</code> to <code>y_train</code> and a placeholder for inputs <code>x</code> to <code>x_train</code>.</p>
<pre class="python" data-language="Python"><code>ed.evaluate(&#39;categorical_accuracy&#39;, data={y_post: y_train, x: x_train})
ed.evaluate(&#39;mean_absolute_error&#39;, data={y_post: y_train, x: x_train})</code></pre>
<p>Point evaluation also applies to unsupervised tasks. For example, we can evaluate the likelihood of observing the data.</p>
<pre class="python" data-language="Python"><code>ed.evaluate(&#39;log_likelihood&#39;, data={x_post: x_train})</code></pre>
<p>It is common practice to criticize models with data held-out from training. To do this, we must first perform inference over any local latent variables of the held-out data, fixing the global variables; we demonstrate this below. Then we make predictions on the held-out data.</p>
<pre class="python" data-language="Python"><code>from edward.models import Categorical

# create local posterior factors for test data, assuming test data
# has N_test many data points
qz_test = Categorical(logits=tf.Variable(tf.zeros[N_test, K]))

# run local inference conditional on global factors
inference_test = ed.Inference({z: qz_test}, data={x: x_test, beta: qbeta})
inference_test.run()

# build posterior predictive on test data
x_post = ed.copy(x, {z: qz_test, beta: qbeta}})
ed.evaluate(&#39;log_likelihood&#39;, data={x_post: x_test})</code></pre>
<p>Point evaluations are formally known as scoring rules in decision theory. Scoring rules are useful for model comparison, model selection, and model averaging.</p>
<p>See the <a href="/api/criticism">criticism API</a> for further details. An example of point evaluation is in the <a href="/tutorials/supervised-regression">supervised learning (regression)</a> tutorial.</p>
<h3 id="posterior-predictive-checks">Posterior predictive checks</h3>
<p>Posterior predictive checks (PPCs) analyze the degree to which data generated from the model deviate from data generated from the true distribution. They can be used either numerically to quantify this degree, or graphically to visualize this degree. PPCs can be thought of as a probabilistic generalization of point evaluation <span class="citation" data-cites="box1980sampling rubin1984bayesianly meng1994posterior gelman1996posterior">(Box, 1980; Gelman, Meng, &amp; Stern, 1996; Meng, 1994; Rubin, 1984)</span>.</p>
<p>The simplest PPC works by applying a test statistic on new data generated from the posterior predictive, such as <span class="math inline">\(T(\mathbf{x}_\text{new}) = \max(\mathbf{x}_\text{new})\)</span>. Applying <span class="math inline">\(T(\mathbf{x}_\text{new})\)</span> to new data over many data replications induces a distribution. We compare this distribution to the test statistic on the real data <span class="math inline">\(T(\mathbf{x})\)</span>.</p>
<p><img src="/images/ppc.png" alt="image" /></p>
<p>In the figure, <span class="math inline">\(T(\mathbf{x})\)</span> falls in a low probability region of this reference distribution: if the model were true, the probability of observing the test statistic is very low. This indicates that the model fits the data poorly according to this check; this suggests an area of improvement for the model.</p>
<p>More generally, the test statistic can be a function of the model’s latent variables <span class="math inline">\(T(\mathbf{x}, \mathbf{z})\)</span>, known as a discrepancy function. Examples of discrepancy functions are the metrics used for point evaluation. We can now interpret the point evaluation as a special case of PPCs: it simply calculates <span class="math inline">\(T(\mathbf{x}, \mathbf{z})\)</span> over the real data and without a reference distribution in mind. A reference distribution allows us to make probabilistic statements about the point, in reference to an overall distribution.</p>
<p>The <code>ed.ppc()</code> method provides a scaffold for studying various discrepancy functions. It takes as input a user-defined discrepancy function, and a data dictionary.</p>
<pre class="python" data-language="Python"><code>ed.ppc(lambda xs, zs: tf.reduce_mean(xs[x_post]), data={x_post: x_train})</code></pre>
<p>The discrepancy can also take latent variables as input, which we pass into the PPC.</p>
<pre class="python" data-language="Python"><code>ed.ppc(lambda xs, zs: tf.maximum(zs[z]),
       data={y_post: y_train, x_ph: x_train},
       latent_vars={z: qz, beta: qbeta})</code></pre>
<p>See the <a href="/api/criticism">criticism API</a> for further details.</p>
<p>PPCs are an excellent tool for revising models—simplifying or expanding the current model as one examines its fit to data. They are inspired by classical hypothesis testing; these methods criticize models under the frequentist perspective of large sample assessment.</p>
<p>PPCs can also be applied to tasks such as hypothesis testing, model comparison, model selection, and model averaging. It’s important to note that while PPCs can be applied as a form of Bayesian hypothesis testing, hypothesis testing is generally not recommended: binary decision making from a single test is not as common a use case as one might believe. We recommend performing many PPCs to get a holistic understanding of the model fit.</p>
<h3 id="references" class="unnumbered">References</h3>
<div id="refs" class="references">
<div id="ref-box1976science">
<p>Box, G. E. (1976). Science and statistics. <em>Journal of the American Statistical Association</em>, <em>71</em>(356), 791–799.</p>
</div>
<div id="ref-box1980sampling">
<p>Box, G. E. (1980). Sampling and Bayes’ inference in scientific modelling and robustness. <em>Journal of the Royal Statistical Society. Series A (General)</em>, 383–430.</p>
</div>
<div id="ref-gelman1996posterior">
<p>Gelman, A., Meng, X.-L., &amp; Stern, H. (1996). Posterior predictive assessment of model fitness via realized discrepancies. <em>Statistica Sinica</em>, 733–760.</p>
</div>
<div id="ref-gneiting2007strictly">
<p>Gneiting, T., &amp; Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. <em>Journal of the American Statistical Association</em>, <em>102</em>(477), 359–378.</p>
</div>
<div id="ref-meng1994posterior">
<p>Meng, X.-L. (1994). Posterior predictive <span class="math inline">\(p\)</span>-values. <em>The Annals of Statistics</em>, 1142–1160.</p>
</div>
<div id="ref-rubin1984bayesianly">
<p>Rubin, D. B. (1984). Bayesianly justifiable and relevant frequency calculations for the applied statistician. <em>The Annals of Statistics</em>, <em>12</em>(4), 1151–1172.</p>
</div>
<div id="ref-winkler1994evaluating">
<p>Winkler, R. L. (1994). Evaluating probabilities: Asymmetric scoring rules. <em>Management Science</em>, <em>40</em>(11), 1395–1405.</p>
</div>
</div>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
