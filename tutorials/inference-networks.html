<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward – Inference Networks</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/getting-started">Getting Started</a>
    <a class="button u-full-width" href="/tutorials/">Tutorials</a>
    <a class="button u-full-width" href="/api/">API</a>
    <a class="button u-full-width" href="/community">Community</a>
    <a class="button u-full-width" href="/contributing">Contributing</a>
    <div class="row" style="padding-bottom: 5%"> </div>
    <a class="button2 u-pull-right" style="padding-right:10%"
      href="https://github.com/blei-lab/edward">
      <span style="vertical-align:middle;">Github</span>&nbsp;
      <img src="/images/github-mark.svg" style="vertical-align:middle;"
      alt="Edward on Github" />
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">

<h2 id="inference-networks">Inference Networks</h2>
<p>An inference network is a flexible construction for parameterizing approximating distributions during inference. They are used in Helmholtz machines <span class="citation" data-cites="dayan1995helmholtz">(Dayan, Hinton, Neal, &amp; Zemel, 1995)</span>, deep Boltzmann machines <span class="citation" data-cites="salakhutdinov2010efficient">(Salakhutdinov &amp; Larochelle, 2010)</span>, and variational auto-encoders <span class="citation" data-cites="kingma2014auto rezende2014stochastic">(Kingma &amp; Welling, 2014; Rezende, Mohamed, &amp; Wierstra, 2014)</span>.</p>
<p>Recall that probabilistic models often have <em>local</em> latent variables, that is, latent variables associated with a data point; for example, the latent cluster assignment of a data point or the hidden representation of a text or image. Variational inference on models with local latent variables requires optimizing over each variational factor, <span class="math display">\[q(\mathbf{z}; \mathbf{\lambda}) = \prod_{n=1}^N q(z_n; \lambda_n),\]</span> where <span class="math inline">\(z_n\)</span> are the latent variables associated to a data point <span class="math inline">\(x_n\)</span>. The set of parameters <span class="math inline">\(\mathbf{\lambda}=\{\lambda_n\}\)</span> grows with the size of data. This makes computation difficult when the data does not fit in memory. Further, a new set of parameters <span class="math inline">\(\mathbf{\lambda}&#39;=\{\lambda_n&#39;\}\)</span> must be optimized at test time, where there may be a new set of data points <span class="math inline">\(\{x_n&#39;\}\)</span>.</p>
<p>An inference network replaces local variational parameters with global parameters. It is a neural network which takes <span class="math inline">\(x_n\)</span> as input and which outputs its local variational parameters <span class="math inline">\(\lambda_n\)</span>, <span class="math display">\[q(\mathbf{z}\mid \mathbf{x}; \theta)
= \prod_{n=1}^N q(z_n \mid x_n; \lambda_n)
= \prod_{n=1}^N q(z_n; \lambda_n = \mathrm{NN}(x_n; \theta)).\]</span> This amortizes inference by only defining a set of <em>global</em> parameters, namely, the parameters of the neural network. These parameters have a fixed size, making the memory complexity of inference a constant. Further, the same set of parameters can be used at test time: regardless of seeing new or old data points <span class="math inline">\(x_n&#39;\)</span>, we simply pass it through the neural network (a forward pass) to obtain its corresponding variational factor <span class="math inline">\(q(z_n&#39;; \lambda_n&#39; =
\mathrm{NN}(x_n&#39;; \theta))\)</span>.</p>
<p>Note that the inference network is an approximation: it is a common misunderstanding that the inference network produces a more expressive variational model. The inference network restricts the size of parameters, meaning it can only do as well as an approximation as the original variational distribution without the inference network.</p>
<p>There are often good reasons beyond computational reasons for wanting to use an inference network, depending on the problem setting. Originating from Helmholtz machines, they are motivated by the hypothesis that &quot;the human perceptual system is a statistical inference engine whose function is to infer the probable causes of sensory input.&quot; The inference network is this function. In cognition, &quot;humans and robots are in the setting of amortized inference: they have to solve many similar inference problems, and can thus offload part of the computational work to shared precomputation and adaptation over time&quot; <span class="citation" data-cites="stuhlmuller2013learning">(Stuhlmüller, Taylor, &amp; Goodman, 2013)</span>.</p>
<h3 id="implementation">Implementation</h3>
<p>Inference networks are easy to build in Edward. In the example below, a data point <code>x</code> is a 28 by 28 pixel image (from MNIST).</p>
<p>We define a TensorFlow placeholder <code>x_ph</code> for the neural network’s input, which is a batch of data points.</p>
<pre class="python" data-language="Python"><code>x_ph = tf.placeholder(tf.float32, [M, 28 * 28])</code></pre>
<p>We build a neural network using Keras. It takes a 28 by 28 pixel image and outputs a <span class="math inline">\(10\)</span>-dimensional output, one for the mean (<code>mu</code>) and one for the standard deviation (<code>sigma</code>).</p>
<pre class="python" data-language="Python"><code>from edward.models import Normal
from keras.layers import Dense

hidden = Dense(256, activation=&#39;relu&#39;)(x_ph)
qz = Normal(loc=Dense(10)(hidden),
            scale=Dense(10, activation=&#39;softplus&#39;)(hidden))</code></pre>
<p>The variational model is parameterized by the neural network’s output.</p>
<p>During each step of inference, we pass in a batch of data points to feed the placeholder. Then we train the variational parameters (inference network’s parameters) according to gradients of the variational objective.</p>
<p>An example script using this variational model can be found at <a href="https://github.com/blei-lab/edward/blob/master/examples/vae.py"><code>examples/vae.py</code></a> in the Github repository. An example with a convolutional architecture can be found at <a href="https://github.com/blei-lab/edward/blob/master/examples/vae_convolutional_prettytensor.py"><code>examples/vae_convolutional_prettytensor.py</code></a> in the Github repository.</p>
<h3 id="footnotes">Footnotes</h3>
<p>Inference networks are also known as recognition models, recognition networks, or inverse mappings.</p>
<p>Variational models parameterized by inference networks are also known as probabilistic encoders, in analogy from coding theory to <a href="/tutorials/decoder">probabilistic decoders</a>. We recommend against this terminology, in favor of making explicit the separation of model and inference. That is, variational inference with inference networks is a general technique that can be applied to models beyond probabilistic decoders.</p>
<p>This tutorial is taken from text originating in <span class="citation" data-cites="tran2016variational">Tran, Ranganath, &amp; Blei (2016)</span>. We thank Kevin Murphy for motivating the tutorial as it is based on our discussions, and also related discussion with Jaan Altosaar.</p>
<h3 id="references" class="unnumbered">References</h3>
<div id="refs" class="references">
<div id="ref-dayan1995helmholtz">
<p>Dayan, P., Hinton, G. E., Neal, R. M., &amp; Zemel, R. S. (1995). The Helmholtz machine. <em>Neural Computation</em>, <em>7</em>(5), 889–904.</p>
</div>
<div id="ref-kingma2014auto">
<p>Kingma, D., &amp; Welling, M. (2014). Auto-encoding variational Bayes. In <em>International conference on learning representations</em>.</p>
</div>
<div id="ref-rezende2014stochastic">
<p>Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In <em>ICML</em> (pp. 1278–1286).</p>
</div>
<div id="ref-salakhutdinov2010efficient">
<p>Salakhutdinov, R., &amp; Larochelle, H. (2010). Efficient learning of deep boltzmann machines. In <em>International conference on artificial intelligence and statistics</em>.</p>
</div>
<div id="ref-stuhlmuller2013learning">
<p>Stuhlmüller, A., Taylor, J., &amp; Goodman, N. (2013). Learning stochastic inverses. In <em>Neural information processing systems</em>.</p>
</div>
<div id="ref-tran2016variational">
<p>Tran, D., Ranganath, R., &amp; Blei, D. M. (2016). The variational gaussian process. In <em>International conference on learning representations</em>.</p>
</div>
</div>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
