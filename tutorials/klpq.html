<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward – KL(p||q) Minimization</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="/css/normalize.css" rel="stylesheet">
<link href="/css/skeleton.css" rel="stylesheet">
<!-- Dynamically resize logo for mobile -->
<style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="/icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="/icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="/icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="/icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="/icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="/icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="/icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="/icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="/icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="/icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="/icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="/icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="/icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="/icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="/icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="/icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="/">Edward</a></h1>
<a href="/">
<center>
<img alt="Edward" class="logo-width" src="/images/edward.png"/>
</center>
</a>
<a class="button u-full-width" href="/getting-started">Getting Started</a>
<a class="button u-full-width" href="/tutorials/">Tutorials</a>
<a class="button u-full-width" href="/api/">API</a>
<a class="button u-full-width" href="/community">Community</a>
<a class="button u-full-width" href="/contributing">Contributing</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a class="button2 u-pull-right" href="https://github.com/blei-lab/edward" style="padding-right:10%">
<span style="vertical-align:middle;">Github</span> 
      <img alt="Edward on Github" src="/images/github-mark.svg" style="vertical-align:middle;"/>
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<h2 id="textklpq-minimization"><span class="math inline">\(\text{KL}(p\|q)\)</span> Minimization</h2>
<p>One form of variational inference minimizes the Kullback-Leibler divergence <strong>from</strong> <span class="math inline">\(p(\mathbf{z} \mid \mathbf{x})\)</span> <strong>to</strong> <span class="math inline">\(q(\mathbf{z}\;;\;\lambda)\)</span>, <span class="math display">\[\begin{aligned}
  \lambda^*
  &amp;=
  \arg\min_\lambda \text{KL}(
  p(\mathbf{z} \mid \mathbf{x})
  \;\|\;
  q(\mathbf{z}\;;\;\lambda)
  )\\
  &amp;=
  \arg\min_\lambda\;
  \mathbb{E}_{p(\mathbf{z} \mid \mathbf{x})}
  \big[
  \log p(\mathbf{z} \mid \mathbf{x})
  -
  \log q(\mathbf{z}\;;\;\lambda)
  \big].\end{aligned}\]</span> The KL divergence is a non-symmetric, information theoretic measure of similarity between two probability distributions.</p>
<h3 id="minimizing-an-intractable-objective-function">Minimizing an intractable objective function</h3>
<p>The <span class="math inline">\(\text{KL}(p\|q)\)</span> objective we seek to minimize is intractable; it directly involves the posterior <span class="math inline">\(p(\mathbf{z} \mid \mathbf{x})\)</span>. Ignoring this for the moment, consider its gradient <span class="math display">\[\begin{aligned}
  \nabla_\lambda\;
  \text{KL}(
  p(\mathbf{z} \mid \mathbf{x})
  \;\|\;
  q(\mathbf{z}\;;\;\lambda)
  )
  &amp;=
  -
  \mathbb{E}_{p(\mathbf{z} \mid \mathbf{x})}
  \big[
  \nabla_\lambda\;
  \log q(\mathbf{z}\;;\;\lambda)
  \big].\end{aligned}\]</span> Both <span class="math inline">\(\text{KL}(p\|q)\)</span> and its gradient are intractable because of the posterior expectation. We can use importance sampling to both estimate the objective and calculate stochastic gradients <span class="citation" data-cites="oh1992adaptive">(Oh &amp; Berger, 1992)</span>.</p>
<h3 id="adaptive-importance-sampling">Adaptive Importance sampling</h3>
<p>First rewrite the expectation to be with respect to the variational distribution, <span class="math display">\[\begin{aligned}
  -
  \mathbb{E}_{p(\mathbf{z} \mid \mathbf{x})}
  \big[
  \nabla_\lambda\;
  \log q(\mathbf{z}\;;\;\lambda)
  \big]
  &amp;=
  -
  \mathbb{E}_{q(\mathbf{z}\;;\;\lambda)}
  \Bigg[
  \frac{p(\mathbf{z} \mid \mathbf{x})}{q(\mathbf{z}\;;\;\lambda)}
  \nabla_\lambda\;
  \log q(\mathbf{z}\;;\;\lambda)
  \Bigg].\end{aligned}\]</span></p>
<p>We then use importance sampling to obtain a noisy estimate of this gradient. The basic procedure follows these steps:</p>
<ol>
<li>draw <span class="math inline">\(S\)</span> samples <span class="math inline">\(\{\mathbf{z}_s\}_1^S \sim q(\mathbf{z}\;;\;\lambda)\)</span>,</li>
<li>evaluate <span class="math inline">\(\nabla_\lambda\; \log q(\mathbf{z}_s\;;\;\lambda)\)</span>,</li>
<li>compute the normalized importance weights <span class="math display">\[\begin{aligned}
    w_s
    &amp;=
    \frac{p(\mathbf{z}_s \mid \mathbf{x})}{q(\mathbf{z}_s\;;\;\lambda)}
    \Bigg/
    \sum_{s=1}^{S}
    \frac{p(\mathbf{z}_s \mid \mathbf{x})}{q(\mathbf{z}_s\;;\;\lambda)}
  \end{aligned}\]</span></li>
<li>compute the weighted sum.</li>
</ol>
<p>The key insight is that we can use the joint <span class="math inline">\(p(\mathbf{x},\mathbf{z})\)</span> instead of the posterior when estimating the normalized importance weights <span class="math display">\[\begin{aligned}
  w_s
  &amp;=
  \frac{p(\mathbf{z}_s \mid \mathbf{x})}{q(\mathbf{z}_s\;;\;\lambda)}
  \Bigg/
  \sum_{s=1}^{S}
  \frac{p(\mathbf{z}_s \mid \mathbf{x})}{q(\mathbf{z}_s\;;\;\lambda)} \\
  &amp;=
  \frac{p(\mathbf{x}, \mathbf{z}_s)}{q(\mathbf{z}_s\;;\;\lambda)}
  \Bigg/
  \sum_{s=1}^{S}
  \frac{p(\mathbf{x}, \mathbf{z}_s)}{q(\mathbf{z}_s\;;\;\lambda)}.\end{aligned}\]</span> This follows from Bayes’ rule <span class="math display">\[\begin{aligned}
  p(\mathbf{z} \mid \mathbf{x})
  &amp;=
  p(\mathbf{x}, \mathbf{z}) / p(\mathbf{x})\\
  &amp;=
  p(\mathbf{x}, \mathbf{z}) / \text{a constant function of }\mathbf{z}.\end{aligned}\]</span></p>
<p>Importance sampling thus gives the following biased yet consistent gradient estimate <span class="math display">\[\begin{aligned}
\nabla_\lambda\;
  \text{KL}(
  p(\mathbf{z} \mid \mathbf{x})
  \;\|\;
  q(\mathbf{z}\;;\;\lambda)
  )
  &amp;=
  -
  \sum_{s=1}^S
  w_s
  \nabla_\lambda\; \log q(\mathbf{z}_s\;;\;\lambda).\end{aligned}\]</span> The objective <span class="math inline">\(\text{KL}(p\|q)\)</span> can be calculated in a similar fashion. The only new ingredient for its gradient is the score function <span class="math inline">\(\nabla_\lambda \log q(\mathbf{z}\;;\;\lambda)\)</span>. Edward uses automatic differentiation, specifically with TensorFlow’s computational graphs, making this gradient computation both simple and efficient to distribute.</p>
<p>Adaptive importance sampling follows this gradient to a local optimum using stochastic optimization. It is adaptive because the variational distribution <span class="math inline">\(q(\mathbf{z}\;;\;\lambda)\)</span> iteratively gets closer to the posterior <span class="math inline">\(p(\mathbf{z} \mid \mathbf{x})\)</span>.</p>
<p>For more details, see the <a href="/api/">API</a> as well as its implementation in Edward’s code base.</p>
<h3 class="unnumbered" id="references">References</h3>
<div class="references" id="refs">
<div id="ref-oh1992adaptive">
<p>Oh, M.-S., &amp; Berger, J. O. (1992). Adaptive importance sampling in Monte Carlo integration. <em>Journal of Statistical Computation and Simulation</em>, <em>41</em>(3-4), 143–168.</p>
</div>
</div>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
