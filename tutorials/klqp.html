<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward – KL(q||p) Minimization</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="/css/normalize.css" rel="stylesheet">
<link href="/css/skeleton.css" rel="stylesheet">
<!-- Dynamically resize logo for mobile -->
<style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="/icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="/icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="/icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="/icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="/icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="/icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="/icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="/icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="/icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="/icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="/icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="/icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="/icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="/icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="/icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="/icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="/">Edward</a></h1>
<a href="/">
<center>
<img alt="Edward" class="logo-width" src="/images/edward.png"/>
</center>
</a>
<a class="button u-full-width" href="/getting-started">Getting Started</a>
<a class="button u-full-width" href="/tutorials/">Tutorials</a>
<a class="button u-full-width" href="/api/">API</a>
<a class="button u-full-width" href="/community">Community</a>
<a class="button u-full-width" href="/contributing">Contributing</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a class="button2 u-pull-right" href="https://github.com/blei-lab/edward" style="padding-right:10%">
<span style="vertical-align:middle;">Github</span> 
      <img alt="Edward on Github" src="/images/github-mark.svg" style="vertical-align:middle;"/>
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<h2 id="textklqp-minimization"><span class="math inline">\(\text{KL}(q\|p)\)</span> Minimization</h2>
<p>One form of variational inference minimizes the Kullback-Leibler divergence <strong>from</strong> <span class="math inline">\(q(\mathbf{z}\;;\;\lambda)\)</span> <strong>to</strong> <span class="math inline">\(p(\mathbf{z} \mid \mathbf{x})\)</span>, <span class="math display">\[\begin{aligned}
  \lambda^*
  &amp;=
  \arg\min_\lambda \text{KL}(
  q(\mathbf{z}\;;\;\lambda)
  \;\|\;
  p(\mathbf{z} \mid \mathbf{x})
  )\\
  &amp;=
  \arg\min_\lambda\;
  \mathbb{E}_{q(\mathbf{z}\;;\;\lambda)}
  \big[
  \log q(\mathbf{z}\;;\;\lambda)
  -
  \log p(\mathbf{z} \mid \mathbf{x})
  \big].\end{aligned}\]</span> The KL divergence is a non-symmetric, information theoretic measure of similarity between two probability distributions <span class="citation" data-cites="hinton1993keeping waterhouse1996bayesian jordan1999introduction">(Hinton &amp; Camp, 1993; Jordan, Ghahramani, Jaakkola, &amp; Saul, 1999; Waterhouse, MacKay, &amp; Robinson, 1996)</span>.</p>
<h3 id="the-evidence-lower-bound">The Evidence Lower Bound</h3>
<p>The above optimization problem is intractable because it directly depends on the posterior <span class="math inline">\(p(\mathbf{z} \mid \mathbf{x})\)</span>. To tackle this, consider the property <span class="math display">\[\begin{aligned}
  \log p(\mathbf{x})
  &amp;=
  \text{KL}(
  q(\mathbf{z}\;;\;\lambda)
  \;\|\;
  p(\mathbf{z} \mid \mathbf{x})
  )\\
  &amp;\quad+\;
  \mathbb{E}_{q(\mathbf{z}\;;\;\lambda)}
  \big[
  \log p(\mathbf{x}, \mathbf{z})
  -
  \log q(\mathbf{z}\;;\;\lambda)
  \big]\end{aligned}\]</span> where the left hand side is the logarithm of the marginal likelihood <span class="math inline">\(p(\mathbf{x}) = \int p(\mathbf{x}, \mathbf{z}) \text{d}\mathbf{z}\)</span>, also known as the model evidence. (Try deriving this using Bayes’ rule!)</p>
<p>The evidence is a constant with respect to the variational parameters <span class="math inline">\(\lambda\)</span>, so we can minimize <span class="math inline">\(\text{KL}(q\|p)\)</span> by instead maximizing the Evidence Lower BOund, <span class="math display">\[\begin{aligned}
  \text{ELBO}(\lambda)
  &amp;=\;
  \mathbb{E}_{q(\mathbf{z}\;;\;\lambda)}
  \big[
  \log p(\mathbf{x}, \mathbf{z})
  -
  \log q(\mathbf{z}\;;\;\lambda)
  \big].\end{aligned}\]</span> In the ELBO, both <span class="math inline">\(p(\mathbf{x}, \mathbf{z})\)</span> and <span class="math inline">\(q(\mathbf{z}\;;\;\lambda)\)</span> are tractable. The optimization problem we seek to solve becomes <span class="math display">\[\begin{aligned}
  \lambda^*
  &amp;=
  \arg \max_\lambda \text{ELBO}(\lambda).\end{aligned}\]</span> As per its name, the ELBO is a lower bound on the evidence, and optimizing it tries to maximize the probability of observing the data. What does maximizing the ELBO do? Splitting the ELBO reveals a trade-off <span class="math display">\[\begin{aligned}
  \text{ELBO}(\lambda)
  &amp;=\;
  \mathbb{E}_{q(\mathbf{z} \;;\; \lambda)}[\log p(\mathbf{x}, \mathbf{z})]
  - \mathbb{E}_{q(\mathbf{z} \;;\; \lambda)}[\log q(\mathbf{z}\;;\;\lambda)],\end{aligned}\]</span> where the first term represents an energy and the second term (including the minus sign) represents the entropy of <span class="math inline">\(q\)</span>. The energy encourages <span class="math inline">\(q\)</span> to focus probability mass where the model puts high probability, <span class="math inline">\(p(\mathbf{x}, \mathbf{z})\)</span>. The entropy encourages <span class="math inline">\(q\)</span> to spread probability mass to avoid concentrating to one location.</p>
<p>Edward uses two generic strategies to obtain gradients for optimization.</p>
<ul>
<li>Score function gradient;</li>
<li>Reparameterization gradient.</li>
</ul>
<h2 id="score-function-gradient">Score function gradient</h2>
<p>Gradient descent is a standard approach for optimizing complicated objectives like the ELBO. The idea is to calculate its gradient <span class="math display">\[\begin{aligned}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &amp;=
  \nabla_\lambda\;
  \mathbb{E}_{q(\mathbf{z}\;;\;\lambda)}
  \big[
  \log p(\mathbf{x}, \mathbf{z})
  -
  \log q(\mathbf{z}\;;\;\lambda)
  \big],\end{aligned}\]</span> and update the current set of parameters proportional to the gradient.</p>
<p>The score function gradient estimator leverages a property of logarithms to write the gradient as <span class="math display">\[\begin{aligned}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &amp;=\;
  \mathbb{E}_{q(\mathbf{z}\;;\;\lambda)}
  \big[
  \nabla_\lambda \log q(\mathbf{z}\;;\;\lambda)
  \:
  \big(
  \log p(\mathbf{x}, \mathbf{z})
  -
  \log q(\mathbf{z}\;;\;\lambda)
  \big)
  \big].\end{aligned}\]</span> The gradient of the ELBO is an expectation over the variational model <span class="math inline">\(q(\mathbf{z}\;;\;\lambda)\)</span>; the only new ingredient it requires is the <em>score function</em> <span class="math inline">\(\nabla_\lambda \log q(\mathbf{z}\;;\;\lambda)\)</span> <span class="citation" data-cites="paisley2012variational ranganath2014black">(Paisley, Blei, &amp; Jordan, 2012; Ranganath, Gerrish, &amp; Blei, 2014)</span>.</p>
<p>We can use Monte Carlo integration to obtain noisy estimates of both the ELBO and its gradient. The basic procedure follows these steps:</p>
<ol>
<li>draw <span class="math inline">\(S\)</span> samples <span class="math inline">\(\{\mathbf{z}_s\}_1^S \sim q(\mathbf{z}\;;\;\lambda)\)</span>,</li>
<li>evaluate the argument of the expectation using <span class="math inline">\(\{\mathbf{z}_s\}_1^S\)</span>, and</li>
<li>compute the empirical mean of the evaluated quantities.</li>
</ol>
<p>A Monte Carlo estimate of the gradient is then <span class="math display">\[\begin{aligned}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &amp;\approx\;
  \frac{1}{S}
  \sum_{s=1}^{S}
  \big[
  \big(
  \log p(\mathbf{x}, \mathbf{z}_s)
  -
  \log q(\mathbf{z}_s\;;\;\lambda)
  \big)
  \:
  \nabla_\lambda \log q(\mathbf{z}_s\;;\;\lambda)
  \big].\end{aligned}\]</span> This is an unbiased estimate of the actual gradient of the ELBO.</p>
<h2 id="reparameterization-gradient">Reparameterization gradient</h2>
<p>If the model has differentiable latent variables, then it is generally advantageous to leverage gradient information from the model in order to better traverse the optimization space. One approach to doing this is the reparameterization gradient <span class="citation" data-cites="kingma2014auto rezende2014stochastic">(Kingma &amp; Welling, 2014; Rezende, Mohamed, &amp; Wierstra, 2014)</span>.</p>
<p>Some variational distributions <span class="math inline">\(q(\mathbf{z}\;;\;\lambda)\)</span> admit useful reparameterizations. For example, we can reparameterize a normal distribution <span class="math inline">\(\mathbf{z} \sim \text{Normal}(\mu, \Sigma)\)</span> as <span class="math inline">\(\mathbf{z} \sim \mu + L \text{Normal}(0, I)\)</span> where <span class="math inline">\(\Sigma = LL^\top\)</span>. In general, write this as <span class="math display">\[\begin{aligned}
  \epsilon &amp;\sim q(\epsilon)\\
  \mathbf{z} &amp;= \mathbf{z}(\epsilon \;;\; \lambda),\end{aligned}\]</span> where <span class="math inline">\(\epsilon\)</span> is a random variable that does <strong>not</strong> depend on the variational parameters <span class="math inline">\(\lambda\)</span>. The deterministic function <span class="math inline">\(\mathbf{z}(\cdot;\lambda)\)</span> encapsulates the variational parameters instead, and following the process is equivalent to directly drawing <span class="math inline">\(\mathbf{z}\)</span> from the original distribution.</p>
<p>The reparameterization gradient leverages this property of the variational distribution to write the gradient as <span class="math display">\[\begin{aligned}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &amp;=\;
  \mathbb{E}_{q(\epsilon)}
  \big[
  \nabla_\lambda
  \big(
  \log p(\mathbf{x}, \mathbf{z}(\epsilon \;;\; \lambda))
  -
  \log q(\mathbf{z}(\epsilon \;;\; \lambda) \;;\;\lambda)
  \big)
  \big].\end{aligned}\]</span> The gradient of the ELBO is an expectation over the base distribution <span class="math inline">\(q(\epsilon)\)</span>, and the gradient can be applied directly to the inner expression.</p>
<p>We can use Monte Carlo integration to obtain noisy estimates of both the ELBO and its gradient. The basic procedure follows these steps:</p>
<ol>
<li>draw <span class="math inline">\(S\)</span> samples <span class="math inline">\(\{\epsilon_s\}_1^S \sim q(\epsilon)\)</span>,</li>
<li>evaluate the argument of the expectation using <span class="math inline">\(\{\epsilon_s\}_1^S\)</span>, and</li>
<li>compute the empirical mean of the evaluated quantities.</li>
</ol>
<p>A Monte Carlo estimate of the gradient is then <span class="math display">\[\begin{aligned}
  \nabla_\lambda\;
  \text{ELBO}(\lambda)
  &amp;\approx\;
  \frac{1}{S}
  \sum_{s=1}^{S}
  \big[
  \nabla_\lambda
  \big(
  \log p(\mathbf{x}, \mathbf{z}(\epsilon_s \;;\; \lambda))
  -
  \log q(\mathbf{z}(\epsilon_s \;;\; \lambda) \;;\;\lambda)
  \big)
  \big].\end{aligned}\]</span> This is an unbiased estimate of the actual gradient of the ELBO. Empirically, it exhibits lower variance than the score function gradient, leading to faster convergence in a large set of problems.</p>
<p>For more details, see the <a href="/api/">API</a> as well as its implementation in Edward’s code base.</p>
<h3 class="unnumbered" id="references">References</h3>
<div class="references" id="refs">
<div id="ref-hinton1993keeping">
<p>Hinton, G. E., &amp; Camp, D. van. (1993). Keeping the neural networks simple by minimizing the description length of the weights. In <em>Conference on learning theory</em>. ACM.</p>
</div>
<div id="ref-jordan1999introduction">
<p>Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., &amp; Saul, L. K. (1999). An introduction to variational methods for graphical models. <em>Machine Learning</em>, <em>37</em>(2), 183–233.</p>
</div>
<div id="ref-kingma2014auto">
<p>Kingma, D., &amp; Welling, M. (2014). Auto-encoding variational Bayes. In <em>International conference on learning representations</em>.</p>
</div>
<div id="ref-paisley2012variational">
<p>Paisley, J., Blei, D. M., &amp; Jordan, M. I. (2012). Variational bayesian inference with stochastic search. In <em>International conference on machine learning</em>.</p>
</div>
<div id="ref-ranganath2014black">
<p>Ranganath, R., Gerrish, S., &amp; Blei, D. (2014). Black box variational inference. In <em>Artificial intelligence and statistics</em>.</p>
</div>
<div id="ref-rezende2014stochastic">
<p>Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In <em>ICML</em> (pp. 1278–1286).</p>
</div>
<div id="ref-waterhouse1996bayesian">
<p>Waterhouse, S., MacKay, D., &amp; Robinson, T. (1996). Bayesian methods for mixtures of experts. <em>Advances in Neural Information Processing Systems</em>, 351–357.</p>
</div>
</div>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
